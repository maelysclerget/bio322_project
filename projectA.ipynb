{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La première partie du projet sera liée à l'étude des data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the train data set is (1300, 131)\n",
      "There is  5 categorial columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7x/43mgl97d0fvdqdc5rb7fhwdc0000gn/T/ipykernel_76084/3215523143.py:39: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.countplot(x=col, data=data_train, palette='viridis')\n",
      "/var/folders/7x/43mgl97d0fvdqdc5rb7fhwdc0000gn/T/ipykernel_76084/3215523143.py:39: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.countplot(x=col, data=data_train, palette='viridis')\n",
      "/var/folders/7x/43mgl97d0fvdqdc5rb7fhwdc0000gn/T/ipykernel_76084/3215523143.py:39: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.countplot(x=col, data=data_train, palette='viridis')\n",
      "/var/folders/7x/43mgl97d0fvdqdc5rb7fhwdc0000gn/T/ipykernel_76084/3215523143.py:39: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.countplot(x=col, data=data_train, palette='viridis')\n",
      "/var/folders/7x/43mgl97d0fvdqdc5rb7fhwdc0000gn/T/ipykernel_76084/3215523143.py:39: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.countplot(x=col, data=data_train, palette='viridis')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre total de noms de samples dupliqués : 234\n",
      "Nombre total de samples concernés 640\n",
      "Number of Infrared Spectrum columns is : 125\n",
      "Graphique sauvegardé : data_visualisation/relation_between_purity_and_spectrum.png\n",
      "Graphique sauvegardé : data_visualisation/spectrum_distribution.png\n",
      "Graphique sauvegardé : data_visualisation/correlation_matrix.png\n",
      "voici les 5 les plus corrélés 908.1_residuals  908.1     1.000000\n",
      "908.1            908.1     1.000000\n",
      "1162.1           1168.3    0.999988\n",
      "1316.9           1310.7    0.999984\n",
      "1397.5           1391.3    0.999976\n",
      "                             ...   \n",
      "1248.8           1670      0.507092\n",
      "1676.2           1242.6    0.506866\n",
      "1645.2           1236.4    0.506376\n",
      "1670             1242.6    0.505995\n",
      "1242.6           1645.2    0.505936\n",
      "Length: 7844, dtype: float64\n",
      "purity mean is 27.02156252675692\n",
      "purity std is 15.67221912592189\n",
      "Graphique sauvegardé : data_visualisation/frequency_purity.png\n",
      "Nombre total de samples avec une pureté < 10 : 85\n",
      "Index: 244, Sample Name: 20.0142-P001\n",
      "Index: 286, Sample Name: 20.0228-P002\n",
      "Index: 407, Sample Name: 20.0228-P001\n",
      "Index: 926, Sample Name: 20.0321-P010\n",
      "Index: 1141, Sample Name: 18.0238-P004\n",
      "Index: 1266, Sample Name: 20.0045-P001.05\n",
      "Nombre total de samples avec une pureté > 60 : 6\n"
     ]
    }
   ],
   "source": [
    "# def knn():\n",
    "# PURITY log transform  \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def data_visualisation():\n",
    "    \n",
    "    # Créer un dossier pour sauvegarder les graphiques\n",
    "    output_dir = 'data_visualisation'\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        print(f\"Répertoire créé : {output_dir}\")\n",
    "        \n",
    "    data_train = pd.read_csv('/Users/alicepriolet/Desktop/ML/epfl-bio-322-2024/train.csv')\n",
    "    data_test = pd.read_csv('/Users/alicepriolet/Desktop/ML/epfl-bio-322-2024/test.csv')\n",
    "    print(\"The size of the train data set is\",data_train.shape)\n",
    "    #print(data_test.shape)\n",
    "    \n",
    "    categorical_columns = data_train.select_dtypes(include=['object', 'category']).columns\n",
    "    print(\"There is \",len(categorical_columns),\"categorial columns\")\n",
    "    \n",
    "        # Répartition des colonnes catégoriques et sauvegarde des graphiques\n",
    "    for col in categorical_columns:\n",
    "        plt.figure(figsize=(15, 15))\n",
    "        sns.countplot(x=col, data=data_train, palette='viridis')\n",
    "        plt.title(f\"Repartition of the categories in {col}\")\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        # Sauvegarder le graphique\n",
    "        graph_path = os.path.join(output_dir, f\"category_distribution_{col}.png\")\n",
    "        plt.savefig(graph_path)\n",
    "        plt.close()\n",
    "        #print(f\"Graphic saved : {graph_path}\")\n",
    "    \n",
    "    # Identify the samples that appear several times in the data set\n",
    "    duplicate_samples = data_train[data_train.duplicated(subset='sample_name', keep=False)]\n",
    "\n",
    "    # Afficher les indices et les noms des échantillons dupliqués\n",
    "    #for index, row in duplicate_samples.iterrows():\n",
    "    #    print(f\"Index: {index}, Sample Name: {row['sample_name']}\")\n",
    "\n",
    "    # Print the total number of duplicated samples\n",
    "    print(f\"Nombre total de noms de samples dupliqués : {duplicate_samples['sample_name'].nunique()}\")\n",
    "    print(\"Nombre total de samples concernés\", len(duplicate_samples))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Count the number of columns of infrared spectrum measurements\n",
    "    spectr_columns = [col for col in data_test.columns if col.startswith('9') or col.startswith('1')]\n",
    "    spectr_count = len(spectr_columns)\n",
    "    \n",
    "    print(f\"Number of Infrared Spectrum columns is : {spectr_count}\")\n",
    "    \n",
    "    # Allows us to see the purity curve \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(data_train['PURITY'], kde=True, bins=30, color='blue')\n",
    "    plt.title(\"Distribution de la variable PURITY\")\n",
    "    plt.xlabel(\"PURITY\")\n",
    "    plt.ylabel(\"Fréquence\")\n",
    "    \n",
    "    graph_path = os.path.join(output_dir, f\"purity_frequence.png\")\n",
    "    plt.savefig(graph_path)\n",
    "    plt.close()\n",
    "    #print(f\"Graphique sauvegardé : {graph_path}\")\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(x=data_train['908.1'], y=data_train['PURITY'], alpha=0.7)\n",
    "    sns.scatterplot(x=data_train['914.3'], y=data_train['PURITY'], alpha=0.7)\n",
    "    sns.scatterplot(x=data_train['920.5'], y=data_train['PURITY'], alpha=0.7)\n",
    "    sns.scatterplot(x=data_train['1316.9'], y=data_train['PURITY'], alpha=0.7)\n",
    "    sns.scatterplot(x=data_train['1676.2'], y=data_train['PURITY'], alpha=0.7)\n",
    "    plt.title(\"Relation between 908.1, 914.3, 920.5. 1316.9 et 1676.2 and Purity\")\n",
    "    plt.xlabel(\"908.1, 914.3, 920.5. 1316.9 and 1676.2\")\n",
    "    plt.ylabel(\"Purity\")\n",
    "    \n",
    "    graph_path = os.path.join(output_dir, f\"relation_between_purity_and_spectrum.png\")\n",
    "    plt.savefig(graph_path)\n",
    "    plt.close()\n",
    "    print(f\"Graphique sauvegardé : {graph_path}\")\n",
    "    \n",
    "\n",
    "    data_train['908.1_residuals'] = data_train['908.1'] - data_train['908.1'].median()\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(data_train['908.1_residuals'], kde=True, bins=30, color='purple')\n",
    "    plt.title(\"Distribution des résidus de 908.1\")\n",
    "    plt.xlabel(\"Résidus de 908.1\")\n",
    "    plt.ylabel(\"Fréquence\")\n",
    "    \n",
    "    graph_path = os.path.join(output_dir, f\"spectrum_distribution.png\")\n",
    "    plt.savefig(graph_path)\n",
    "    plt.close()\n",
    "    print(f\"Graphique sauvegardé : {graph_path}\")\n",
    "\n",
    "    data_reduced = data_train.iloc[:, 6:]\n",
    "    corr_matrix = data_reduced.corr()  # Matrice de corrélation\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(corr_matrix, cmap='coolwarm', annot=False, cbar=True)\n",
    "    plt.title(\"Matrice de corrélation des variables numériques\")\n",
    "    \n",
    "    graph_path = os.path.join(output_dir, f\"correlation_matrix.png\")\n",
    "    plt.savefig(graph_path)\n",
    "    plt.close()\n",
    "    print(f\"Graphique sauvegardé : {graph_path}\")\n",
    "\n",
    "    top_5_correlations = (\n",
    "    corr_matrix\n",
    "    .abs()  # Prendre les valeurs absolues des corrélations\n",
    "    .unstack()  # Convertir en série pour itérer\n",
    "    .sort_values(ascending=False)  # Trier par corrélation décroissante\n",
    "    .drop_duplicates()  # Supprimer les doublons\n",
    "    )\n",
    "\n",
    "    # Filtrer pour exclure les corrélations de la diagonale (valeur de corrélation = 1)\n",
    "    top_5_corr = top_5_correlations[top_5_correlations < 1].head(5)\n",
    "\n",
    "    print(\"voici les 5 les plus corrélés\",top_5_correlations)\n",
    "    \n",
    "    mean_purity = data_train['PURITY'].mean()\n",
    "    std_purity= data_train['PURITY'].std()\n",
    "    \n",
    "    print('purity mean is',mean_purity)\n",
    "    print('purity std is',std_purity)\n",
    "    \n",
    "    # Create histogram\n",
    "    sns.kdeplot(data = data_train, x = 'PURITY', fill = True)\n",
    "    plt.title('Purity distribution')\n",
    "    plt.xlabel('Purity')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    graph_path = os.path.join(output_dir, f\"frequency_purity.png\")\n",
    "    plt.savefig(graph_path)\n",
    "    plt.close()\n",
    "    print(f\"Graphique sauvegardé : {graph_path}\")\n",
    "    \n",
    "    # Filtrer et afficher les noms des échantillons ayant une pureté < 10\n",
    "    low_purity_samples = data_train[data_train['PURITY'] < 10]['sample_name']\n",
    "\n",
    "    # Afficher les noms des échantillons\n",
    "    #for index, sample in low_purity_samples.items():\n",
    "        #print(f\"Index: {index}, Sample Name: {sample}\")\n",
    "        \n",
    "    print(f\"Nombre total de samples avec une pureté < 10 : {len(low_purity_samples)}\")\n",
    "    \n",
    "    # Filtrer et afficher les noms des échantillons ayant une pureté > 60\n",
    "    high_purity_samples = data_train[data_train['PURITY'] > 60]['sample_name']\n",
    "\n",
    "    # Afficher les noms des échantillons\n",
    "    for index, sample in high_purity_samples.items():\n",
    "        print(f\"Index: {index}, Sample Name: {sample}\")\n",
    "        \n",
    "    print(f\"Nombre total de samples avec une pureté > 60 : {len(high_purity_samples)}\")\n",
    "    \n",
    "\n",
    "\n",
    "def preprocessing_v1(apply_one_hot=False, apply_scaling=True, apply_pca=False, apply_correlation=False, apply_remove_outliers=True):\n",
    "    train_data_og = pd.read_csv('/Users/alicepriolet/Desktop/ML/epfl-bio-322-2024/train.csv')\n",
    "    test_data_og = pd.read_csv('/Users/alicepriolet/Desktop/ML/epfl-bio-322-2024/test.csv')\n",
    "    train_data = train_data_og.copy()\n",
    "    test_data = test_data_og.copy()\n",
    "    train_data = train_data.drop(columns=['prod_substance'])\n",
    "    test_data = test_data.drop(columns=['prod_substance'])\n",
    "    \n",
    "    non_wavelength_cols = ['device_serial', 'substance_form_display', 'measure_type_display']\n",
    "    wavelength_cols = train_data.columns[6:]\n",
    "    \n",
    "    if apply_one_hot:\n",
    "        # One Hot encoding \n",
    "        encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "        X_train_encoded = encoder.fit_transform(train_data[non_wavelength_cols])\n",
    "        X_test_encoded = encoder.transform(test_data[non_wavelength_cols])\n",
    "        \n",
    "        # Convert encoded features to DataFrame\n",
    "        X_train_encoded_df = pd.DataFrame(X_train_encoded, columns=encoder.get_feature_names_out(non_wavelength_cols))\n",
    "        X_test_encoded_df = pd.DataFrame(X_test_encoded, columns=encoder.get_feature_names_out(non_wavelength_cols))\n",
    "        \n",
    "        train_data_combined = pd.concat([pd.DataFrame(X_train_encoded_df), train_data[wavelength_cols].reset_index(drop=True)], axis=1)\n",
    "        test_data_combined = pd.concat([pd.DataFrame(X_test_encoded_df), test_data[wavelength_cols].reset_index(drop=True)], axis=1)\n",
    "    else:\n",
    "        train_data_combined = train_data\n",
    "        test_data_combined = test_data\n",
    "    \n",
    "    # Add sample_name column back to the combined DataFrames\n",
    "    train_data_combined.insert(0, 'sample_name', train_data_og['sample_name'])\n",
    "    test_data_combined.insert(0, 'sample_name', test_data_og['sample_name'])\n",
    "    \n",
    "    # Remove NaN values\n",
    "    train_data_combined = train_data_combined.dropna()\n",
    "    test_data_combined = test_data_combined.dropna()\n",
    "    \n",
    "    y_train = train_data['PURITY'].iloc[train_data_combined.index]\n",
    "    \n",
    "    #NOTE: toujours mettre scaling avec remove_outliers\n",
    "    \n",
    "    if apply_scaling:\n",
    "        # Standardize the data\n",
    "        scaler = StandardScaler()\n",
    "        wavelength_train_scaled = scaler.fit_transform(train_data_combined[wavelength_cols])\n",
    "        wavelength_test_scaled = scaler.transform(test_data_combined[wavelength_cols])\n",
    "        \n",
    "        train_data_combined[wavelength_cols] = wavelength_train_scaled\n",
    "        test_data_combined[wavelength_cols] = wavelength_test_scaled\n",
    "    \n",
    "    if apply_remove_outliers:\n",
    "        # Remove outliers\n",
    "        outliers_index = (np.abs(wavelength_train_scaled) > 3).any(axis=1)\n",
    "        train_data_combined = train_data_combined[~outliers_index]\n",
    "        y_train = y_train[~outliers_index]\n",
    "        \n",
    "        train_data_combined = train_data_combined.reset_index(drop=True)\n",
    "        test_data_combined = test_data_combined.reset_index(drop=True)\n",
    "        y_train = y_train.reset_index(drop=True)\n",
    "        \n",
    "    if apply_pca:\n",
    "        # Perform PCA on scaled wavelength columns\n",
    "        pca = PCA(n_components=5)\n",
    "        wavelength_cols = train_data_combined.columns[54:]\n",
    "        \n",
    "        X_train_pca = pca.fit_transform(train_data_combined[wavelength_cols])\n",
    "        X_test_pca = pca.transform(test_data_combined[wavelength_cols])\n",
    "\n",
    "        # Combine PCA components with original data\n",
    "        X_train_combined = pd.concat([train_data_combined.iloc[:, :54].reset_index(drop=True), \n",
    "                                      pd.DataFrame(X_train_pca, columns=[f'PC{i+1}' for i in range(5)])], axis=1)\n",
    "        X_test_combined = pd.concat([test_data_combined.iloc[:, :54].reset_index(drop=True), \n",
    "                                     pd.DataFrame(X_test_pca, columns=[f'PC{i+1}' for i in range(5)])], axis=1)\n",
    "        \n",
    "        train_data_combined = X_train_combined\n",
    "        test_data_combined = X_test_combined\n",
    "        \n",
    "    if apply_correlation:\n",
    "    # Compute correlation matrix only for wavelength columns\n",
    "        wavelength_cols = train_data_combined.columns[54:] #pq t'as mis ca\n",
    "        correlation_matrix = train_data_combined[wavelength_cols].corr()\n",
    "\n",
    "        # Visualize correlation matrix\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.heatmap(correlation_matrix, cmap='coolwarm', annot=False)\n",
    "        plt.title(\"Correlation Matrix for Wavelength Features\")\n",
    "        plt.show()\n",
    "\n",
    "        # Identify highly correlated features (e.g., |r| > 0.999)\n",
    "        threshold_high = 0.999\n",
    "        threshold_low = 0.2\n",
    "\n",
    "        high_corr_pairs = [\n",
    "            (i, j)\n",
    "            for i in range(correlation_matrix.shape[0])\n",
    "            for j in range(i + 1, correlation_matrix.shape[1])\n",
    "            if abs(correlation_matrix.iloc[i, j]) > threshold_high\n",
    "        ]\n",
    "        \n",
    "        features_to_drop = set()\n",
    "        for i, j in high_corr_pairs:\n",
    "            features_to_drop.add(wavelength_cols[j])  # Arbitrarily drop the second feature in the pair\n",
    "\n",
    "        # Remove the selected features\n",
    "        train_data_combined = train_data_combined.drop(columns=list(features_to_drop))\n",
    "        test_data_combined = test_data_combined.drop(columns=list(features_to_drop))\n",
    "        \n",
    "        low_corr_pairs = [\n",
    "            (i, j)\n",
    "            for i in range(correlation_matrix.shape[0])\n",
    "            for j in range(i + 1, correlation_matrix.shape[1])\n",
    "            if abs(correlation_matrix.iloc[i, j]) < threshold_low\n",
    "        ]\n",
    "\n",
    "        print(\"Highly correlated features:\")\n",
    "        for i, j in high_corr_pairs:\n",
    "            print(f\"{wavelength_cols[i]} and {wavelength_cols[j]}: {correlation_matrix.iloc[i, j]}\")\n",
    "        print(\"Low correlated features:\")\n",
    "        for i, j in low_corr_pairs:\n",
    "            print(f\"{wavelength_cols[i]} and {wavelength_cols[j]}: {correlation_matrix.iloc[i, j]}\")\n",
    "    \n",
    "    \n",
    "    return train_data_combined, test_data_combined, y_train\n",
    "\n",
    "   \n",
    "def calculate_feature_importance(X_train, y_train, X_test, threshold=0.25):\n",
    "    # Calculate feature importance using a linear regression model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Print feature importance\n",
    "    feature_importance = pd.Series(model.coef_, index=X_train.columns)\n",
    "    feature_importance = feature_importance.abs().sort_values(ascending=False)\n",
    "    wavelength_feature_importance_df = feature_importance.reset_index()\n",
    "    wavelength_feature_importance_df.columns = ['Feature', 'Importance']\n",
    "    wavelength_feature_importance_df.to_csv('/Users/alicepriolet/Desktop/ML/epfl-bio-322-2024/feature_importance_LR1.csv', index=False)\n",
    "    print('Feature Importance saved successfully.')\n",
    "    \n",
    "    # Calculate stats threshold\n",
    "    threshold_value = feature_importance.quantile(threshold)\n",
    "    \n",
    "    # Identify low-importance features\n",
    "    low_importance_features = feature_importance[feature_importance < threshold_value].index\n",
    "    print(f'Low importance features: {low_importance_features}')\n",
    "    \n",
    "    # Remove low-importance features\n",
    "    X_train_reduced = X_train.drop(columns=low_importance_features)\n",
    "    X_test_reduced = X_test.drop(columns=low_importance_features)\n",
    "    \n",
    "    return X_train_reduced, X_test_reduced\n",
    "\n",
    "def submission_file(y_test_predicted):\n",
    "    submission_reduced = pd.DataFrame({\n",
    "        'ID': range(1, len(y_test_predicted) + 1),\n",
    "        'PURITY': y_test_predicted\n",
    "    })\n",
    "    return submission_reduced\n",
    "\n",
    "\n",
    "def linear_regression(feature_importance=False):\n",
    "    X_train, X_test, y_train = preprocessing_v1(apply_one_hot=True, apply_correlation=True, apply_scaling=True, apply_remove_outliers=True)\n",
    "    X_train = X_train.drop(columns=['sample_name'])\n",
    "    X_test = X_test.drop(columns=['sample_name'])\n",
    "    \n",
    "    if feature_importance:\n",
    "        X_train, X_test = calculate_feature_importance(X_train, y_train, X_test)\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_train_pred = model.predict(X_train)\n",
    "    mse = mean_squared_error(y_train, y_train_pred)\n",
    "    print('Training MSE:', mse)\n",
    "    \n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "    print('CV MSE:', -cv_scores.mean())\n",
    "    \n",
    "    # Predict on test data\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Create submission DataFrame\n",
    "    submission = submission_file(y_test_pred)\n",
    "    \n",
    "    # Save submission to CSV\n",
    "    submission.to_csv('/Users/alicepriolet/Desktop/ML/epfl-bio-322-2024/sample_submission_LR.csv', index=False)\n",
    "    print('Submission file saved successfully.')\n",
    "\n",
    "def polynomial_regression():\n",
    "    X_train, X_test, y_train = preprocessing_v1(apply_one_hot=True, apply_scaling=True, remove_outliers=True)\n",
    "    X_train = X_train.drop(columns=['sample_name'])\n",
    "    X_test = X_test.drop(columns=['sample_name'])\n",
    "    \n",
    "    # Define the pipeline\n",
    "    pipeline = Pipeline([\n",
    "        (\"polynomial\", PolynomialFeatures()),\n",
    "        (\"regression\", LinearRegression())\n",
    "    ])\n",
    "    \n",
    "    # Define the parameter grid\n",
    "    param_grid = {\n",
    "        \"polynomial__degree\": np.arange(1, 4, 1)\n",
    "    }\n",
    "    \n",
    "    # Initialize GridSearchCV\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=10, \n",
    "                               scoring='neg_mean_squared_error', \n",
    "                               return_train_score=True)\n",
    "    \n",
    "    # Fit GridSearchCV\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "    print(\"Best accuracy:\", grid_search.best_score_)\n",
    "    print(\"Best parameters:\", grid_search.best_params_)\n",
    "    \n",
    "    # Predict on training data\n",
    "    y_train_pred = best_model.predict(X_train)\n",
    "    mse = mean_squared_error(y_train, y_train_pred)\n",
    "    print('Training MSE:', mse)\n",
    "    \n",
    "    # Cross-validation score\n",
    "    cv_scores = cross_val_score(best_model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "    print('CV MSE:', -cv_scores.mean())\n",
    "    \n",
    "    # Predict on test data\n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "    \n",
    "    # Create submission DataFrame\n",
    "    submission = submission_file(y_test_pred)\n",
    "    \n",
    "    # Save submission to CSV\n",
    "    submission.to_csv('/Users/alicepriolet/Desktop/ML/epfl-bio-322-2024/sample_submission_POLY.csv', index=False)\n",
    "    print('Submission file saved successfully.')\n",
    "\n",
    "\n",
    "def logistic_regression_with_preprocessed_data():\n",
    "    \"\"\"\n",
    "    Implémente une régression logistique sur des données prétraitées\n",
    "    avec preprocessing_v1. Évalue le modèle et génère un fichier\n",
    "    de soumission.\n",
    "    \"\"\"\n",
    "    # Appliquer le prétraitement\n",
    "    X_train, X_test, y_train = preprocessing_v1(\n",
    "        apply_one_hot=True,\n",
    "        apply_scaling=True,\n",
    "        apply_remove_outliers=True,\n",
    "        apply_pca=False,  # Désactiver PCA pour garder toutes les dimensions\n",
    "        apply_correlation=False  # Désactiver le nettoyage par corrélation\n",
    "    )\n",
    "\n",
    "    # Supprimer la colonne 'sample_name' inutilisée\n",
    "    X_train = X_train.drop(columns=['sample_name'], errors='ignore')\n",
    "    X_test = X_test.drop(columns=['sample_name'], errors='ignore')\n",
    "\n",
    "    # Binariser la cible pour une tâche de classification binaire\n",
    "    threshold = 0.5  # Seuil pour séparer les classes\n",
    "    y_train_binary = (y_train > threshold).astype(int)  # 0 si ≤ seuil, 1 si > seuil\n",
    "\n",
    "    # Initialiser et entraîner le modèle de régression logistique\n",
    "    model = LogisticRegression(max_iter=1000, solver='lbfgs')\n",
    "    model.fit(X_train, y_train_binary)\n",
    "\n",
    "    # Évaluer les performances sur les données d'entraînement\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    train_accuracy = accuracy_score(y_train_binary, y_train_pred)\n",
    "    print(\"Training Accuracy:\", train_accuracy)\n",
    "    print(\"\\nClassification Report (Training):\")\n",
    "    print(classification_report(y_train_binary, y_train_pred))\n",
    "\n",
    "    # Calculer l'AUC (Area Under Curve)\n",
    "    y_train_proba = model.predict_proba(X_train)[:, 1]\n",
    "    auc = roc_auc_score(y_train_binary, y_train_proba)\n",
    "    print(f\"Training AUC: {auc}\")\n",
    "\n",
    "    # Prédictions sur l'ensemble de test\n",
    "    y_test_pred = model.predict_proba(X_test)[:, 1]  # Probabilités pour la classe positive\n",
    "\n",
    "    # Générer le fichier de soumission\n",
    "    submission = submission_file(y_test_pred)\n",
    "    output_dir = '/Users/alicepriolet/Desktop/ML/epfl-bio-322-2024'\n",
    "    submission_path = os.path.join(output_dir, 'sample_submission_Logistic.csv') \n",
    "    submission.to_csv(submission_path, index=False)\n",
    "    print(f\"Submission file saved successfully \")\n",
    "    \n",
    "    \n",
    "\n",
    "def main():\n",
    "    data_visualisation()\n",
    "    #linear_regression(feature_importance=True)\n",
    "    #polynomial_regression()\n",
    "    #logistic_regression_with_preprocessed_data()\n",
    "    #inspect_data_grouped_and_save('/Users/alicepriolet/Desktop/ML/epfl-bio-322-2024/train.csv')\n",
    "    \n",
    " \n",
    "   \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLCourse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
