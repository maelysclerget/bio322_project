{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'sample_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/MLCourse/lib/python3.10/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'sample_name'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 455\u001b[0m\n\u001b[1;32m    452\u001b[0m     xg_boost()\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 455\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n",
      "Cell \u001b[0;32mIn[43], line 452\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;66;03m#preparation_data_substances()\u001b[39;00m\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;66;03m#preparation_data_train()\u001b[39;00m\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;66;03m#merge_csv_files()\u001b[39;00m\n\u001b[1;32m    451\u001b[0m     preparation_data_test()\n\u001b[0;32m--> 452\u001b[0m     \u001b[43mxg_boost\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[43], line 422\u001b[0m, in \u001b[0;36mxg_boost\u001b[0;34m()\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mxg_boost\u001b[39m():\n\u001b[0;32m--> 422\u001b[0m     X_train, X_test, y_train \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessing_v1\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    424\u001b[0m     target_column \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPURITY\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    427\u001b[0m     param_grid \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    428\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolsample_bytree\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m5\u001b[39m),\n\u001b[1;32m    429\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubsample\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m5\u001b[39m),\n\u001b[1;32m    430\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    431\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[43], line 403\u001b[0m, in \u001b[0;36mpreprocessing_v1\u001b[0;34m(apply_one_hot, apply_scaling, apply_pca, apply_correlation, apply_remove_outliers, apply_variance_threshold, apply_random_forest)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" # we obtain the names of the unwanted features\u001b[39;00m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;124;03m    dropped_feature_names = X_train_rf.columns[feature_indices]\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;124;03m    train_data_combined = train_data_combined.drop(columns=dropped_feature_names)\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;124;03m    test_data_combined = test_data_combined.drop(columns=dropped_feature_names) \"\"\"\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;66;03m# Add sample_name column back to the combined DataFrames\u001b[39;00m\n\u001b[0;32m--> 403\u001b[0m train_data_combined\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_name\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mtrain_data_og\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msample_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m    404\u001b[0m test_data_combined\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_name\u001b[39m\u001b[38;5;124m'\u001b[39m, test_data_og[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_name\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    405\u001b[0m y_train \u001b[38;5;241m=\u001b[39m train_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPURITY\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[train_data_combined\u001b[38;5;241m.\u001b[39mindex]\n",
      "File \u001b[0;32m~/anaconda3/envs/MLCourse/lib/python3.10/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/anaconda3/envs/MLCourse/lib/python3.10/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'sample_name'"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "from xgboost import XGBRegressor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from scipy.stats import chi2\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def round_column_names(precision=1):\n",
    "    \"\"\"\n",
    "    Arrondit les noms des colonnes numériques à une précision donnée\n",
    "    et les convertit en entiers si l'arrondi donne un nombre entier.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame avec les colonnes à renommer.\n",
    "        precision (int): Précision de l'arrondi (par défaut : 1 décimale).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame avec les noms de colonnes arrondis.\n",
    "    \"\"\"\n",
    "    # Vérifiez que df est bien un DataFrame\n",
    "    df = pd.read_csv('/Users/alicepriolet/Desktop/ML/epfl-bio-322-2024/substances.csv')\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        raise TypeError(f\"L'entrée doit être un DataFrame, mais {type(df)} a été fourni.\")\n",
    "\n",
    "    # Nouveau dictionnaire pour mapper les anciens noms aux nouveaux\n",
    "    new_columns = {}\n",
    "    for col in df.columns:\n",
    "        try:\n",
    "            # Tenter de convertir le nom de la colonne en float pour arrondi\n",
    "            rounded_col = round(float(col), precision)\n",
    "            # Si l'arrondi donne un entier, convertir en entier\n",
    "            if rounded_col.is_integer():\n",
    "                new_columns[col] = str(int(rounded_col))\n",
    "            else:\n",
    "                new_columns[col] = str(rounded_col)\n",
    "        except ValueError:\n",
    "            # Conserver les colonnes non convertibles (par exemple, chaînes)\n",
    "            new_columns[col] = col\n",
    "\n",
    "    # Renommer les colonnes dans le DataFrame\n",
    "    return df.rename(columns=new_columns)\n",
    "\n",
    "\n",
    "def add_purity_column():\n",
    "    \"\"\"\n",
    "    Ajoute une colonne 'PURITY' remplie de 0 au dataset substances.csv.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Chemin vers le fichier CSV d'entrée.\n",
    "        output_path (str): Chemin pour sauvegarder le fichier CSV avec la colonne ajoutée.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame avec la colonne 'PURITY' ajoutée.\n",
    "    \"\"\"\n",
    "    # Charger le fichier CSV\n",
    "    data = round_column_names()\n",
    "\n",
    "    # Ajouter la colonne 'PURITY' avec des valeurs remplies à 0\n",
    "    data.insert(1, 'PURITY', 0)\n",
    "    \n",
    "    output_path = '/Users/alicepriolet/Desktop/ML/epfl-bio-322-2024/substances_modified.csv'\n",
    "\n",
    "    # Sauvegarder le fichier mis à jour\n",
    "    data.to_csv(output_path, index=False)\n",
    "\n",
    "    return data\n",
    "\n",
    "def add_measure_type_display():\n",
    "    data = add_purity_column()\n",
    "    data.insert(0, 'measure_type_display', 'Direct contact') \n",
    "    output_path = '/Users/alicepriolet/Desktop/ML/epfl-bio-322-2024/substances_modified.csv'\n",
    "\n",
    "    # Sauvegarder le fichier mis à jour\n",
    "    data.to_csv(output_path, index=False)\n",
    "\n",
    "    return data\n",
    " \n",
    "def add_substance_form_display():\n",
    "    data = add_measure_type_display()\n",
    "    data.insert(0, 'substance_form_display', 'Unspecified') \n",
    "    output_path = '/Users/alicepriolet/Desktop/ML/epfl-bio-322-2024/substances_modified.csv'\n",
    "\n",
    "    # Sauvegarder le fichier mis à jour\n",
    "    data.to_csv(output_path, index=False)\n",
    "\n",
    "    return data\n",
    "        \n",
    "def add_device_serial():\n",
    "    data = add_substance_form_display()\n",
    "    data.insert(0, 'device_serial', 'M1-1000112') \n",
    "    output_path = '/Users/alicepriolet/Desktop/ML/epfl-bio-322-2024/substances_modified.csv'\n",
    "\n",
    "    # Sauvegarder le fichier mis à jour\n",
    "    data.to_csv(output_path, index=False)\n",
    "\n",
    "    return data\n",
    "\n",
    "def remove_first_column():\n",
    "    \"\"\"\n",
    "    Supprime la première colonne d'un fichier CSV et sauvegarde le fichier mis à jour.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Chemin vers le fichier CSV d'entrée.\n",
    "        output_path (str): Chemin pour sauvegarder le fichier CSV mis à jour.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame avec la première colonne supprimée.\n",
    "    \"\"\"\n",
    "    # Charger le fichier CSV\n",
    "    data = add_purity_column()\n",
    "    output_path = '/Users/alicepriolet/Desktop/ML/epfl-bio-322-2024/substances_modified.csv'\n",
    "    # Supprimer la première colonne\n",
    "    data = data.drop(columns='substance')\n",
    "\n",
    "    # Sauvegarder le fichier mis à jour\n",
    "    data.to_csv(output_path, index=False)\n",
    "\n",
    "    return data\n",
    "\n",
    "def preparation_data_substances():\n",
    "    remove_first_column()\n",
    "    \n",
    "\n",
    "def preparation_data_train():\n",
    "    data = df = pd.read_csv('/Users/alicepriolet/Desktop/ML/epfl-bio-322-2024/train.csv')\n",
    "    non_wavelength_cols = ['sample_name','device_serial', 'substance_form_display', 'measure_type_display','prod_substance']\n",
    "    data = data.drop(columns=non_wavelength_cols,errors='ignore')\n",
    "    #data.drop('sample_name')\n",
    "    print(data.head())\n",
    "    \n",
    "    output_path = '/Users/alicepriolet/Desktop/ML/epfl-bio-322-2024/train_modified.csv'\n",
    "    data.to_csv(output_path, index=False)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def preparation_data_test(): \n",
    "    data = pd.read_csv('/Users/alicepriolet/Desktop/ML/epfl-bio-322-2024/test.csv')\n",
    "    non_wavelength_cols = ['sample_name','device_serial', 'substance_form_display', 'measure_type_display','prod_substance']\n",
    "    data = data.drop(columns=non_wavelength_cols,errors='ignore')\n",
    "    #data.drop('sample_name')\n",
    "    #print(data.head())\n",
    "    \n",
    "    output_path = '/Users/alicepriolet/Desktop/ML/epfl-bio-322-2024/test_modified.csv'\n",
    "    data.to_csv(output_path, index=False)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def merge_csv_files():\n",
    "    \"\"\"\n",
    "    Combine les lignes de deux fichiers CSV et sauvegarde le résultat dans un nouveau fichier.\n",
    "\n",
    "    Args:\n",
    "        train_file (str): Chemin vers le fichier train.csv.\n",
    "        substances_file (str): Chemin vers le fichier substances.csv.\n",
    "        output_file (str): Chemin pour sauvegarder le fichier combiné.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame combiné.\n",
    "    \"\"\"\n",
    "    # Charger les deux fichiers CSV\n",
    "    train_data = pd.read_csv('/Users/alicepriolet/Desktop/ML/epfl-bio-322-2024/train_modified.csv')\n",
    "    substances_data = pd.read_csv('/Users/alicepriolet/Desktop/ML/epfl-bio-322-2024/substances_modified.csv')\n",
    "    \n",
    "\n",
    "    # Combiner les données\n",
    "    combined_data = pd.concat([train_data, substances_data], ignore_index=False)\n",
    "\n",
    "    # Sauvegarder le fichier combiné\n",
    "    output_file = '/Users/alicepriolet/Desktop/ML/epfl-bio-322-2024/train_final.csv'\n",
    "    combined_data.to_csv(output_file, index=False)\n",
    "\n",
    "    return combined_data\n",
    "\n",
    "def submission_file(y_test_predicted):\n",
    "    submission_reduced = pd.DataFrame({\n",
    "        'ID': range(1, len(y_test_predicted) + 1),\n",
    "        'PURITY': y_test_predicted\n",
    "    })\n",
    "    return submission_reduced\n",
    "\n",
    "def preprocessing_v1(apply_one_hot=False, apply_scaling=False, apply_pca=False, apply_correlation=False, apply_remove_outliers=False, apply_variance_threshold=False, apply_random_forest=False):\n",
    "    train_data_og = pd.read_csv('/Users/alicepriolet/Desktop/ML/epfl-bio-322-2024/train_final.csv')\n",
    "    test_data_og = pd.read_csv('/Users/alicepriolet/Desktop/ML/epfl-bio-322-2024/test_modified.csv')\n",
    "    train_data = train_data_og.copy()\n",
    "    test_data = test_data_og.copy()\n",
    "    #train_data = train_data.drop(columns=['prod_substance'])\n",
    "    #test_data = test_data.drop(columns=['prod_substance'])\n",
    "    \n",
    "    non_wavelength_cols = ['device_serial', 'substance_form_display', 'measure_type_display']\n",
    "    wavelength_cols = train_data.columns[5:]\n",
    "    \n",
    "    # Remove NaN values\n",
    "    train_data = train_data.dropna()\n",
    "    test_data = test_data.dropna()\n",
    "    \n",
    "    if apply_one_hot:\n",
    "        # One Hot encoding \n",
    "        encoder = OneHotEncoder(drop='first',sparse_output=False, handle_unknown='ignore')\n",
    "        X_train_encoded = encoder.fit_transform(train_data[non_wavelength_cols])\n",
    "        X_test_encoded = encoder.transform(test_data[non_wavelength_cols])\n",
    "        \n",
    "        # Convert encoded features to DataFrame\n",
    "        X_train_encoded_df = pd.DataFrame(X_train_encoded, columns=encoder.get_feature_names_out(non_wavelength_cols))\n",
    "        X_test_encoded_df = pd.DataFrame(X_test_encoded, columns=encoder.get_feature_names_out(non_wavelength_cols))\n",
    "        \n",
    "        train_data_combined = pd.concat([pd.DataFrame(X_train_encoded_df), train_data[wavelength_cols].reset_index(drop=True)], axis=1)\n",
    "        test_data_combined = pd.concat([pd.DataFrame(X_test_encoded_df), test_data[wavelength_cols].reset_index(drop=True)], axis=1)\n",
    "    else:\n",
    "        train_data_combined = train_data\n",
    "        test_data_combined = test_data  \n",
    "        \n",
    "    if apply_remove_outliers:\n",
    "        \n",
    "        non_outlier_indices = remove_outliers_mahalanobis(train_data_combined[wavelength_cols]).index\n",
    "        train_data_combined = train_data_combined.loc[non_outlier_indices].reset_index(drop=True)\n",
    "        print(f\"After Mahalanobis outlier removal, train data shape: {train_data_combined.shape}\") \n",
    "            \n",
    "    if apply_scaling:\n",
    "         # Standardisers\n",
    "        train_data_std = StandardScaler().fit(train_data_combined[wavelength_cols].values)\n",
    "\n",
    "        # Standardise the data\n",
    "        wavelength_train_scaled, wavelength_test_scaled = map(\n",
    "            lambda data, std_mach: std_mach.transform(data),\n",
    "            [\n",
    "                train_data_combined[wavelength_cols].values,\n",
    "                test_data_combined[wavelength_cols].values,\n",
    "            ],\n",
    "            [train_data_std, train_data_std],\n",
    "        )     \n",
    "        \n",
    "        train_data_combined[wavelength_cols] = pd.DataFrame(wavelength_train_scaled, columns=wavelength_cols)\n",
    "        test_data_combined[wavelength_cols] = pd.DataFrame(wavelength_test_scaled, columns=wavelength_cols)   \n",
    "        \n",
    "    if apply_pca:\n",
    "        # Perform PCA on scaled wavelength columns\n",
    "        pca = PCA(n_components=5)\n",
    "        wavelength_cols = train_data_combined.columns[54:]\n",
    "        \n",
    "        X_train_pca = pca.fit_transform(train_data_combined[wavelength_cols])\n",
    "        X_test_pca = pca.transform(test_data_combined[wavelength_cols])\n",
    "\n",
    "        # Combine PCA components with original data\n",
    "        X_train_combined = pd.concat([train_data_combined.iloc[:, :54].reset_index(drop=True), \n",
    "                                      pd.DataFrame(X_train_pca, columns=[f'PC{i+1}' for i in range(5)])], axis=1)\n",
    "        X_test_combined = pd.concat([test_data_combined.iloc[:, :54].reset_index(drop=True), \n",
    "                                     pd.DataFrame(X_test_pca, columns=[f'PC{i+1}' for i in range(5)])], axis=1)\n",
    "        \n",
    "        train_data_combined = X_train_combined\n",
    "        test_data_combined = X_test_combined\n",
    "        \n",
    "    \"\"\" if apply_random_forest:\n",
    "        # Apply Random Forest for feature selection\n",
    "        wavelength_cols = train_data_combined.columns[50:] \n",
    "        rf = RandomForestRegressor(n_estimators=300, random_state=42)\n",
    "        y_train = train_data['PURITY'].iloc[train_data_combined.index]\n",
    "        rf.fit(train_data_combined[wavelength_cols], y_train)\n",
    "        \n",
    "        # Select features based on importance\n",
    "        selector = SelectFromModel(rf, threshold=\"mean\", prefit=True)\n",
    "        train_data_combined = pd.DataFrame(selector.transform(train_data_combined[wavelength_cols]), \n",
    "                                           columns=train_data_combined[wavelength_cols].columns[selector.get_support()])\n",
    "        test_data_combined = pd.DataFrame(selector.transform(test_data_combined[wavelength_cols]), \n",
    "                                          columns=test_data_combined[wavelength_cols].columns[selector.get_support()])\n",
    "        print(f\"Shape after Random Forest feature selection: {train_data_combined.shape}\")  \"\"\"\n",
    "    \n",
    "    if apply_variance_threshold:\n",
    "        # Apply VarianceThreshold\n",
    "        selector = VarianceThreshold(threshold=0.05)\n",
    "        train_data_combined = pd.DataFrame(selector.fit_transform(train_data_combined), columns=train_data_combined.columns[selector.get_support(indices=True)])\n",
    "        test_data_combined = pd.DataFrame(selector.transform(test_data_combined), columns=test_data_combined.columns[selector.get_support(indices=True)])\n",
    "        print(f\"Shape after VarianceThreshold: {train_data_combined.shape}\")\n",
    "    \n",
    "    #Aussi tester Random forest à la placde de correlation matrix\n",
    "    if apply_correlation:\n",
    "    # Compute correlation matrix only for wavelength columns\n",
    "        #wavelength_cols = train_data_combined.columns[50:] \n",
    "        #correlation_matrix = train_data_combined[wavelength_cols].corr()\n",
    "        correlation_matrix = train_data_combined.corr()\n",
    "\n",
    "        # Visualize correlation matrix\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.heatmap(correlation_matrix, cmap='coolwarm', annot=False)\n",
    "        plt.title(\"Correlation Matrix for All Features\")\n",
    "        plt.show()\n",
    "\n",
    "        # Identify highly correlated features (e.g., |r| > 0.999)\n",
    "        threshold_high = 0.9999\n",
    "\n",
    "        print(f\"Number of features before removing highly correlated features: {train_data_combined.shape[1]}\")\n",
    "        \n",
    "        high_corr_pairs = [\n",
    "            (i, j)\n",
    "            for i in range(correlation_matrix.shape[0])\n",
    "            for j in range(i + 1, correlation_matrix.shape[1])\n",
    "            if abs(correlation_matrix.iloc[i, j]) > threshold_high\n",
    "        ]\n",
    "        \n",
    "        features_to_drop = set()\n",
    "        for i, j in high_corr_pairs:\n",
    "            features_to_drop.add(correlation_matrix.columns[j])  # Arbitrarily drop the second feature in the pair\n",
    "\n",
    "        # Remove the selected features\n",
    "        train_data_combined = train_data_combined.drop(columns=list(features_to_drop))\n",
    "        test_data_combined = test_data_combined.drop(columns=list(features_to_drop))\n",
    "        \n",
    "        #wavelength_cols = train_data_combined.columns[50:]\n",
    "        \n",
    "        print(f\"Number of features after removing highly correlated features: {train_data_combined.shape[1]}\")\n",
    "        \"\"\" print(\"Highly correlated features:\")\n",
    "        for i, j in high_corr_pairs:\n",
    "            print(f\"{correlation_matrix.columns[i]} and {correlation_matrix.columns[j]}: {correlation_matrix.iloc[i, j]}\") \"\"\"\n",
    "        \n",
    "    if apply_random_forest:\n",
    "        \n",
    "        wavelength_cols = train_data_combined.columns[50:]\n",
    "        X_train_rf = train_data_combined[wavelength_cols]\n",
    "        y_train_rf = train_data['PURITY'].iloc[train_data_combined.index].squeeze()\n",
    "\n",
    "        # Create and train random forest model\n",
    "        rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "        rf_model.fit(X_train_rf, y_train_rf)\n",
    "\n",
    "        # Perform feature selection using the specified threshold\n",
    "        sfm = SelectFromModel(rf_model, threshold=0.0048, prefit=True)\n",
    "        selected_features = sfm.get_support()\n",
    "        selected_feature_names = X_train_rf.columns[selected_features]\n",
    "\n",
    "        # Apply feature selection to train and test data\n",
    "        train_data_selected = train_data_combined[selected_feature_names]\n",
    "        test_data_selected = test_data_combined[selected_feature_names]\n",
    "\n",
    "        # Add back non-wavelength columns if needed\n",
    "        non_wavelength_cols = [col for col in train_data_combined.columns if col not in wavelength_cols]\n",
    "        train_data_combined = pd.concat([train_data_combined[non_wavelength_cols], train_data_selected], axis=1)\n",
    "        test_data_combined = pd.concat([test_data_combined[non_wavelength_cols], test_data_selected], axis=1)\n",
    "\n",
    "        print(f\"Selected {len(selected_feature_names)} features using Random Forest with threshold {0.0048}.\")\n",
    "\n",
    "        # Evaluate feature importances\n",
    "        feature_importances = rf_model.feature_importances_\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(range(len(feature_importances)), feature_importances)\n",
    "        plt.title(\"Feature Importances\")\n",
    "        plt.xlabel(\"Feature Index\")\n",
    "        plt.ylabel(\"Importance Score\")\n",
    "        plt.show()\n",
    "\n",
    "        \"\"\" # Test different thresholds\n",
    "        thresholds = [0.0025, 0.003, 0.0035, 0.004, 0.0045, 0.0048, 0.005, 0.0052]\n",
    "        cross_val_scores = []\n",
    "\n",
    "        for threshold in thresholds:\n",
    "            # Select features based on threshold\n",
    "            sfm = SelectFromModel(rf_model, threshold=threshold, prefit=True)\n",
    "            selected_features = sfm.get_support()\n",
    "            selected_feature_names = X_train_rf.columns[selected_features]\n",
    "\n",
    "            # Subset the dataset\n",
    "            X_train_selected = X_train_rf[selected_feature_names]\n",
    "\n",
    "            # Compute cross-validation scores\n",
    "            scores = cross_val_score(rf_model, X_train_selected, y_train_rf, cv=5, scoring='r2')\n",
    "            mean_score = scores.mean()\n",
    "            cross_val_scores.append(mean_score)\n",
    "\n",
    "            print(f\"Threshold: {threshold}\")\n",
    "            print(f\"Number of selected features: {len(selected_feature_names)}\")\n",
    "            print(f\"Cross-validated R^2 score: {mean_score:.4f}\")\n",
    "\n",
    "        # Plot cross-validated R^2 scores vs. thresholds\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(thresholds, cross_val_scores, marker='o')\n",
    "        plt.title(\"Cross-Validated R^2 Score vs. Threshold\")\n",
    "        plt.xlabel(\"Threshold\")\n",
    "        plt.ylabel(\"Mean R^2 Score\")\n",
    "        plt.grid()\n",
    "        plt.show() \"\"\"\n",
    "\n",
    "\n",
    "        \"\"\" # we obtain the names of the unwanted features\n",
    "        dropped_feature_names = X_train_rf.columns[feature_indices]\n",
    "        \n",
    "        train_data_combined = train_data_combined.drop(columns=dropped_feature_names)\n",
    "        test_data_combined = test_data_combined.drop(columns=dropped_feature_names) \"\"\"\n",
    "\n",
    "            \n",
    "    # Add sample_name column back to the combined DataFrames\n",
    "    train_data_combined.insert(0, 'sample_name', train_data_og['sample_name'])\n",
    "    test_data_combined.insert(0, 'sample_name', test_data_og['sample_name'])\n",
    "    y_train = train_data['PURITY'].iloc[train_data_combined.index]\n",
    "\n",
    "    print(f\"Shape of OG train data: {train_data_og.shape}\")\n",
    "    print(f\"Shape of OG test data: {test_data_og.shape}\")\n",
    "    print(f\"Shape of train data: {train_data_combined.shape}\")\n",
    "    print(f\"Shape of test data: {test_data_combined.shape}\")\n",
    "    print(f\"Shape of y_train: {y_train.shape}\")\n",
    "    \n",
    "    train_data_combined.to_csv('/Users/maelysclerget/Desktop/ML/bio322_project/epfl-bio-322-2024/train_data_combined.csv', index=False)\n",
    "    print('Submission file saved successfully.')\n",
    "    test_data_combined.to_csv('/Users/maelysclerget/Desktop/ML/bio322_project/epfl-bio-322-2024/test_data_combined.csv', index=False)\n",
    "    print('Submission file saved successfully.')\n",
    "            \n",
    "    return train_data_combined, test_data_combined, y_train\n",
    "\n",
    "def xg_boost():\n",
    "    \n",
    "    X_train, X_test, y_train = preprocessing_v1()\n",
    "    \n",
    "    target_column = 'PURITY'\n",
    "\n",
    "\n",
    "    param_grid = {\n",
    "        'colsample_bytree': np.linspace(0.5, 1, 5),\n",
    "        'subsample': np.linspace(0.5, 1, 5),\n",
    "        'max_depth': np.arange(2, 7, 1)\n",
    "    }\n",
    "\n",
    "\n",
    "    model = XGBRegressor(n_estimators=200, max_depth=10, learning_rate=0.1, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Create submission DataFrame\n",
    "    submission = submission_file(y_pred)\n",
    "    \n",
    "    # Save submission to CSV\n",
    "    submission.to_csv('/Users/alicepriolet/Desktop/ML/epfl-bio-322-2024/sample_submission_LR.csv', index=False)\n",
    "    print('Submission file saved successfully.')\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    #preparation_data_substances()\n",
    "    #preparation_data_train()\n",
    "    #merge_csv_files()\n",
    "    preparation_data_test()\n",
    "    xg_boost()\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main() \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLCourse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
