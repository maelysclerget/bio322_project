{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "y contains previously unseen labels: '19.0278-P012'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/MLCourse/lib/python3.10/site-packages/sklearn/utils/_encode.py:225\u001b[0m, in \u001b[0;36m_encode\u001b[0;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_map_to_integer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muniques\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/MLCourse/lib/python3.10/site-packages/sklearn/utils/_encode.py:165\u001b[0m, in \u001b[0;36m_map_to_integer\u001b[0;34m(values, uniques)\u001b[0m\n\u001b[1;32m    164\u001b[0m table \u001b[38;5;241m=\u001b[39m _nandict({val: i \u001b[38;5;28;01mfor\u001b[39;00m i, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(uniques)})\n\u001b[0;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([table[v] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values])\n",
      "File \u001b[0;32m~/anaconda3/envs/MLCourse/lib/python3.10/site-packages/sklearn/utils/_encode.py:165\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    164\u001b[0m table \u001b[38;5;241m=\u001b[39m _nandict({val: i \u001b[38;5;28;01mfor\u001b[39;00m i, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(uniques)})\n\u001b[0;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([\u001b[43mtable\u001b[49m\u001b[43m[\u001b[49m\u001b[43mv\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values])\n",
      "File \u001b[0;32m~/anaconda3/envs/MLCourse/lib/python3.10/site-packages/sklearn/utils/_encode.py:159\u001b[0m, in \u001b[0;36m_nandict.__missing__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnan_value\n\u001b[0;32m--> 159\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: '19.0278-P012'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 923\u001b[0m\n\u001b[1;32m    920\u001b[0m     linear_regression()\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 923\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n",
      "Cell \u001b[0;32mIn[56], line 920\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[1;32m    912\u001b[0m     \u001b[38;5;66;03m#add_substance_form_display()\u001b[39;00m\n\u001b[1;32m    913\u001b[0m     \u001b[38;5;66;03m#rename_and_modify_substance_column()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;66;03m#preprocessing_v1()\u001b[39;00m\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;66;03m#xg_boost()\u001b[39;00m\n\u001b[0;32m--> 920\u001b[0m     \u001b[43mlinear_regression\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[56], line 881\u001b[0m, in \u001b[0;36mlinear_regression\u001b[0;34m(feature_importance, apply_y_transformation)\u001b[0m\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlinear_regression\u001b[39m(feature_importance\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, apply_y_transformation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 881\u001b[0m     X_train, X_test, y_train \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessing_vX\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapply_one_hot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapply_scaling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    882\u001b[0m     X_train \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_name\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    883\u001b[0m     X_test \u001b[38;5;241m=\u001b[39m X_test\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_name\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[56], line 839\u001b[0m, in \u001b[0;36mpreprocessing_vX\u001b[0;34m(apply_one_hot, apply_scaling, enable_categorical)\u001b[0m\n\u001b[1;32m    837\u001b[0m             le \u001b[38;5;241m=\u001b[39m LabelEncoder()\n\u001b[1;32m    838\u001b[0m             train_data[col] \u001b[38;5;241m=\u001b[39m le\u001b[38;5;241m.\u001b[39mfit_transform(train_data[col]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m))\n\u001b[0;32m--> 839\u001b[0m             test_data[col] \u001b[38;5;241m=\u001b[39m \u001b[43mle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[38;5;66;03m# Normalisation des colonnes numériques (longueurs d'onde)\u001b[39;00m\n\u001b[1;32m    842\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m apply_scaling:\n",
      "File \u001b[0;32m~/anaconda3/envs/MLCourse/lib/python3.10/site-packages/sklearn/preprocessing/_label.py:137\u001b[0m, in \u001b[0;36mLabelEncoder.transform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _num_samples(y) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([])\n\u001b[0;32m--> 137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muniques\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses_\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/MLCourse/lib/python3.10/site-packages/sklearn/utils/_encode.py:227\u001b[0m, in \u001b[0;36m_encode\u001b[0;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _map_to_integer(values, uniques)\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 227\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my contains previously unseen labels: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check_unknown:\n",
      "\u001b[0;31mValueError\u001b[0m: y contains previously unseen labels: '19.0278-P012'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "from xgboost import XGBRegressor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from scipy.stats import chi2\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def round_column_names(precision=1):\n",
    "    \"\"\"\n",
    "    Arrondit les noms des colonnes numériques à une précision donnée\n",
    "    et les convertit en entiers si l'arrondi donne un nombre entier.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame avec les colonnes à renommer.\n",
    "        precision (int): Précision de l'arrondi (par défaut : 1 décimale).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame avec les noms de colonnes arrondis.\n",
    "    \"\"\"\n",
    "    # Vérifiez que df est bien un DataFrame\n",
    "    df = pd.read_csv('/Users/alicepriolet/Desktop/ML/epfl-bio-322-2024/substances.csv')\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        raise TypeError(f\"L'entrée doit être un DataFrame, mais {type(df)} a été fourni.\")\n",
    "\n",
    "    # Nouveau dictionnaire pour mapper les anciens noms aux nouveaux\n",
    "    new_columns = {}\n",
    "    for col in df.columns:\n",
    "        try:\n",
    "            # Tenter de convertir le nom de la colonne en float pour arrondi\n",
    "            rounded_col = round(float(col), precision)\n",
    "            # Si l'arrondi donne un entier, convertir en entier\n",
    "            if rounded_col.is_integer():\n",
    "                new_columns[col] = str(int(rounded_col))\n",
    "            else:\n",
    "                new_columns[col] = str(rounded_col)\n",
    "        except ValueError:\n",
    "            # Conserver les colonnes non convertibles (par exemple, chaînes)\n",
    "            new_columns[col] = col\n",
    "\n",
    "    # Renommer les colonnes dans le DataFrame\n",
    "    return df.rename(columns=new_columns)\n",
    "\n",
    "def add_purity_column():\n",
    "    \"\"\"\n",
    "    Ajoute une colonne 'PURITY' remplie de 0 au dataset substances.csv.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Chemin vers le fichier CSV d'entrée.\n",
    "        output_path (str): Chemin pour sauvegarder le fichier CSV avec la colonne ajoutée.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame avec la colonne 'PURITY' ajoutée.\n",
    "    \"\"\"\n",
    "    # Charger le fichier CSV\n",
    "    data = round_column_names()\n",
    "\n",
    "    # Ajouter la colonne 'PURITY' avec des valeurs remplies à 0\n",
    "    data.insert(1, 'PURITY', 0)\n",
    "    \n",
    "    output_path = '/Users/alicepriolet/Desktop/ML/epfl-bio-322-2024/substances_modified.csv'\n",
    "\n",
    "    # Sauvegarder le fichier mis à jour\n",
    "    data.to_csv(output_path, index=False)\n",
    "\n",
    "    return data\n",
    "\n",
    "def add_measure_type_display():\n",
    "    data = add_purity_column()\n",
    "    data.insert(0, 'measure_type_display', 'Direct contact') \n",
    "    output_path = '/Users/alicepriolet/Desktop/ML/epfl-bio-322-2024/substances_modified.csv'\n",
    "\n",
    "    # Sauvegarder le fichier mis à jour\n",
    "    data.to_csv(output_path, index=False)\n",
    "\n",
    "    return data\n",
    "\n",
    "def add_substance_form_display():\n",
    "    \"\"\"\n",
    "    Ajoute une colonne 'substance_form_display' remplie selon une distribution spécifiée\n",
    "    et l'insère avant toute colonne existante nommée 'substance_form_display'.\n",
    "\n",
    "    La distribution est :\n",
    "    - 'Homogenized Powder' : 630/1250\n",
    "    - 'Non Homogenized Powder' : 200/1400\n",
    "    - 'Unspecified' : 420/1250\n",
    "    \"\"\"\n",
    "    # Appel de la fonction existante pour récupérer les données\n",
    "    data = add_measure_type_display()\n",
    "\n",
    "    # Définir les catégories et leurs probabilités\n",
    "    categories = ['Homogenized Powder', 'Non Homogenized Powder', 'Unspecified']\n",
    "    probabilities = [630/1250, 200/1250, 420/1250]\n",
    "\n",
    "    # Générer les valeurs aléatoires pour la nouvelle colonne\n",
    "    new_column_values = np.random.choice(categories, size=len(data), p=probabilities)\n",
    "\n",
    "    # Trouver l'index de la colonne 'substance_form_display' existante\n",
    "    insert_position = data.columns.get_loc('measure_type_display')\n",
    "\n",
    "    # Insérer la nouvelle colonne avant l'ancienne\n",
    "    data.insert(insert_position, 'substance_form_display', new_column_values)\n",
    "\n",
    "    # Chemin de sortie\n",
    "    output_path = '/Users/alicepriolet/Desktop/ML/epfl-bio-322-2024/substances_modified.csv'\n",
    "\n",
    "    # Sauvegarder le fichier mis à jour\n",
    "    data.to_csv(output_path, index=False)\n",
    "\n",
    "    print(f\"Fichier sauvegardé avec la colonne 'substance_form_display' ajoutée avant la position existante : {output_path}\")\n",
    "    return data\n",
    "\n",
    "def rename_and_modify_substance_column():\n",
    "    \"\"\"\n",
    "    Extrait la colonne 'substance', la renomme en 'prod_substance', \n",
    "    remplace toutes les valeurs par 'Heroin', et sauvegarde le fichier mis à jour.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Chemin vers le fichier CSV d'entrée.\n",
    "        output_path (str): Chemin pour sauvegarder le fichier modifié.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame avec la colonne modifiée.\n",
    "    \"\"\"\n",
    "    # Charger le fichier CSV\n",
    "    data = add_substance_form_display()\n",
    "\n",
    "    # Vérifier si la colonne 'substance' existe\n",
    "    if 'substance' not in data.columns:\n",
    "        raise ValueError(\"La colonne 'substance' n'existe pas dans le fichier.\")\n",
    "\n",
    "    # Renommer la colonne en 'prod_substance' et remplacer toutes les valeurs par 'Heroin'\n",
    "    data = data.rename(columns={'substance': 'prod_substance'})\n",
    "    data['prod_substance'] = 'Heroin'\n",
    "\n",
    "    # Sauvegarder le fichier modifié\n",
    "    output_path = '/Users/alicepriolet/Desktop/ML/epfl-bio-322-2024/substances_modified.csv'\n",
    "    data.to_csv(output_path, index=False)\n",
    "\n",
    "    print(f\"Fichier sauvegardé avec la colonne 'prod_substance' modifiée : {output_path}\")\n",
    "    return data\n",
    "\n",
    "def device_serial_proba():\n",
    "    \"\"\"\n",
    "    Calcule les probabilités de chaque catégorie dans la colonne 'device_serial'.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Chemin vers le fichier CSV.\n",
    "\n",
    "    Returns:\n",
    "        categories (list): Liste des catégories uniques dans 'device_serial'.\n",
    "        probabilities (list): Liste des probabilités associées à chaque catégorie.\n",
    "    \"\"\"\n",
    "    # Charger les données\n",
    "    data, data2, data3 = preprocessing_v1()\n",
    "\n",
    "    # Calculer la répartition et les probabilités\n",
    "    device_serial_distribution = data['device_serial'].value_counts(normalize=True)\n",
    "    categories = device_serial_distribution.index.tolist()\n",
    "    probabilities = device_serial_distribution.values.tolist()\n",
    "\n",
    "    return categories, probabilities\n",
    "\n",
    "def add_device_serial():\n",
    "    \"\"\"\n",
    "    Ajoute une colonne 'device_serial' remplie selon une distribution de probabilité\n",
    "    et insère cette colonne avant 'substance_form_display'.\n",
    "\n",
    "    Args:\n",
    "        input_file (str): Chemin du fichier d'entrée.\n",
    "        output_file (str): Chemin pour sauvegarder le fichier modifié.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame mis à jour.\n",
    "    \"\"\"\n",
    "    # Charger les données existantes\n",
    "    data = rename_and_modify_substance_column()\n",
    "\n",
    "    # Obtenir les catégories et probabilités de 'device_serial'\n",
    "    categories, probabilities = device_serial_proba()\n",
    "\n",
    "    # Générer les valeurs aléatoires pour la nouvelle colonne\n",
    "    new_column_values = np.random.choice(categories, size=len(data), p=probabilities)\n",
    "\n",
    "    # Trouver la position de 'substance_form_display'\n",
    "    if 'substance_form_display' in data.columns:\n",
    "        insert_position = data.columns.get_loc('substance_form_display')\n",
    "    else:\n",
    "        raise ValueError(\"La colonne 'substance_form_display' n'existe pas dans le fichier.\")\n",
    "\n",
    "    # Insérer la nouvelle colonne avant 'substance_form_display'\n",
    "    data.insert(insert_position, 'device_serial', new_column_values)\n",
    "\n",
    "    # Sauvegarder le fichier mis à jour\n",
    "    output_path = '/Users/alicepriolet/Desktop/ML/epfl-bio-322-2024/substances_modified.csv'\n",
    "    data.to_csv(output_path, index=False)\n",
    "    print(f\"Fichier sauvegardé avec la colonne 'device_serial' ajoutée \")\n",
    "    \n",
    "    return data\n",
    "\n",
    "def sample_name_proba():\n",
    "    \"\"\"\n",
    "    Calcule les probabilités de chaque catégorie dans la colonne 'device_serial'.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Chemin vers le fichier CSV.\n",
    "\n",
    "    Returns:\n",
    "        categories (list): Liste des catégories uniques dans 'device_serial'.\n",
    "        probabilities (list): Liste des probabilités associées à chaque catégorie.\n",
    "    \"\"\"\n",
    "    # Charger les données\n",
    "    data1, data2, data3 = preprocessing_v1()\n",
    "\n",
    "    # Calculer la répartition et les probabilités\n",
    "    sample_name_distribution = data1['sample_name'].value_counts(normalize=True)\n",
    "    categories = sample_name_distribution.index.tolist()\n",
    "    probabilities = sample_name_distribution.values.tolist()\n",
    "\n",
    "    return categories, probabilities\n",
    "\n",
    "def add_sample_name(): \n",
    "    \"\"\"\n",
    "    Ajoute une colonne 'device_serial' remplie selon une distribution de probabilité\n",
    "    et insère cette colonne avant 'substance_form_display'.\n",
    "\n",
    "    Args:\n",
    "        input_file (str): Chemin du fichier d'entrée.\n",
    "        output_file (str): Chemin pour sauvegarder le fichier modifié.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame mis à jour.\n",
    "    \"\"\"\n",
    "    # Charger les données existantes\n",
    "    data = add_device_serial()\n",
    "\n",
    "    # Obtenir les catégories et probabilités de 'device_serial'\n",
    "    categories, probabilities = sample_name_proba()\n",
    "\n",
    "    # Générer les valeurs aléatoires pour la nouvelle colonne\n",
    "    new_column_values = np.random.choice(categories, size=len(data), p=probabilities)\n",
    "\n",
    "    # Trouver la position de 'device_serial'\n",
    "    if 'device_serial' in data.columns:\n",
    "        insert_position = data.columns.get_loc('device_serial')\n",
    "    else:\n",
    "        raise ValueError(\"La colonne 'device_serial' n'existe pas dans le fichier.\")\n",
    "\n",
    "    # Insérer la nouvelle colonne avant 'substance_form_display'\n",
    "    data.insert(insert_position, 'sample_name', new_column_values)\n",
    "\n",
    "    # Sauvegarder le fichier mis à jour\n",
    "    output_path = '/Users/alicepriolet/Desktop/ML/epfl-bio-322-2024/substances_modified.csv'\n",
    "    data.to_csv(output_path, index=False)\n",
    "    print(f\"Fichier sauvegardé avec la colonne 'sample_name' ajoutée \")\n",
    "    return data        \n",
    "\n",
    "def merge_csv_files():\n",
    "    \"\"\"\n",
    "    Combine les lignes de deux fichiers CSV et sauvegarde le résultat dans un nouveau fichier.\n",
    "\n",
    "    Args:\n",
    "        train_file (str): Chemin vers le fichier train.csv.\n",
    "        substances_file (str): Chemin vers le fichier substances.csv.\n",
    "        output_file (str): Chemin pour sauvegarder le fichier combiné.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame combiné.\n",
    "    \"\"\"\n",
    "    # Charger les deux fichiers CSV\n",
    "    train_data, data2, data3 = preprocessing_v1()\n",
    "    substances_data = pd.read_csv('/Users/alicepriolet/Desktop/ML/epfl-bio-322-2024/substances_modified.csv')\n",
    "    \n",
    "\n",
    "    # Combiner les données\n",
    "    combined_data = pd.concat([train_data, substances_data], ignore_index=False)\n",
    "\n",
    "    # Sauvegarder le fichier combiné\n",
    "    output_file = '/Users/alicepriolet/Desktop/ML/epfl-bio-322-2024/train_final.csv'\n",
    "    combined_data.to_csv(output_file, index=False)\n",
    "\n",
    "    return combined_data\n",
    " \n",
    "def remove_outliers_mahalanobis(data, threshold=0.99):\n",
    "    \"\"\"\n",
    "    Remove multivariate outliers using Mahalanobis distance.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: DataFrame of numeric features.\n",
    "    - threshold: Chi-squared threshold for outlier removal (default 0.99).\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame without outliers.\n",
    "    \"\"\"\n",
    "    cov_matrix = np.cov(data, rowvar=False)\n",
    "    inv_cov_matrix = np.linalg.inv(cov_matrix)\n",
    "    mean_vec = data.mean(axis=0)\n",
    "    \n",
    "    def mahalanobis(x):\n",
    "        diff = x - mean_vec\n",
    "        return np.sqrt(diff.T @ inv_cov_matrix @ diff)\n",
    "    \n",
    "    mahalanobis_distances = data.apply(mahalanobis, axis=1)\n",
    "    chi2_threshold = chi2.ppf(threshold, df=data.shape[1])\n",
    "    non_outliers = mahalanobis_distances <= np.sqrt(chi2_threshold)\n",
    "    \n",
    "    return data[non_outliers].reset_index(drop=True)\n",
    "\n",
    "def preprocessing_v1(apply_one_hot=False, apply_scaling=False, apply_pca=False, apply_correlation=False, apply_remove_outliers=False, apply_variance_threshold=False, apply_random_forest=False, enable_categorical = False):\n",
    "    train_data_og = pd.read_csv('/Users/alicepriolet/Desktop/ML/epfl-bio-322-2024/train_final.csv')\n",
    "    test_data_og = pd.read_csv('/Users/alicepriolet/Desktop/ML/epfl-bio-322-2024/test.csv')\n",
    "    train_data = train_data_og.copy()\n",
    "    test_data = test_data_og.copy()\n",
    "    train_data = train_data.drop(columns=['prod_substance'])\n",
    "    test_data = test_data.drop(columns=['prod_substance'])\n",
    "    train_data = train_data.drop(columns=['sample_name'])\n",
    "    test_data = test_data.drop(columns=['sample_name'])\n",
    "    non_wavelength_cols = ['device_serial', 'substance_form_display', 'measure_type_display']\n",
    "    wavelength_cols = train_data.columns[5:]\n",
    "    \n",
    "    # Remove NaN values\n",
    "    train_data = train_data.dropna()\n",
    "    test_data = test_data.dropna()\n",
    "    \n",
    "    if apply_one_hot:\n",
    "        # One Hot encoding \n",
    "        encoder = OneHotEncoder(drop='first',sparse_output=False, handle_unknown='ignore')\n",
    "        X_train_encoded = encoder.fit_transform(train_data[non_wavelength_cols])\n",
    "        X_test_encoded = encoder.transform(test_data[non_wavelength_cols])\n",
    "        \n",
    "        # Convert encoded features to DataFrame\n",
    "        X_train_encoded_df = pd.DataFrame(X_train_encoded, columns=encoder.get_feature_names_out(non_wavelength_cols))\n",
    "        X_test_encoded_df = pd.DataFrame(X_test_encoded, columns=encoder.get_feature_names_out(non_wavelength_cols))\n",
    "        \n",
    "        train_data_combined = pd.concat([pd.DataFrame(X_train_encoded_df), train_data[wavelength_cols].reset_index(drop=True)], axis=1)\n",
    "        test_data_combined = pd.concat([pd.DataFrame(X_test_encoded_df), test_data[wavelength_cols].reset_index(drop=True)], axis=1)\n",
    "    else:\n",
    "        train_data_combined = train_data\n",
    "        test_data_combined = test_data  \n",
    "        \n",
    "    if apply_remove_outliers:\n",
    "        \n",
    "        non_outlier_indices = remove_outliers_mahalanobis(train_data_combined[wavelength_cols]).index\n",
    "        train_data_combined = train_data_combined.loc[non_outlier_indices].reset_index(drop=True)\n",
    "        print(f\"After Mahalanobis outlier removal, train data shape: {train_data_combined.shape}\") \n",
    "            \n",
    "    if apply_scaling:\n",
    "         # Standardisers\n",
    "        train_data_std = StandardScaler().fit(train_data_combined[wavelength_cols].values)\n",
    "\n",
    "        # Standardise the data\n",
    "        wavelength_train_scaled, wavelength_test_scaled = map(\n",
    "            lambda data, std_mach: std_mach.transform(data),\n",
    "            [\n",
    "                train_data_combined[wavelength_cols].values,\n",
    "                test_data_combined[wavelength_cols].values,\n",
    "            ],\n",
    "            [train_data_std, train_data_std],\n",
    "        )     \n",
    "        \n",
    "        train_data_combined[wavelength_cols] = pd.DataFrame(wavelength_train_scaled, columns=wavelength_cols)\n",
    "        test_data_combined[wavelength_cols] = pd.DataFrame(wavelength_test_scaled, columns=wavelength_cols)   \n",
    "    \n",
    "    if enable_categorical:\n",
    "        # Convertir les colonnes en type `category` pour XGBoost\n",
    "        for col in non_wavelength_cols:\n",
    "            if col in train_data.columns:\n",
    "                train_data[col] = train_data[col].astype('category')\n",
    "            if col in test_data.columns:\n",
    "                test_data[col] = test_data[col].astype('category')\n",
    "    else:\n",
    "        # Encodage des colonnes catégoriques avec LabelEncoder\n",
    "        for col in non_wavelength_cols:\n",
    "            if col in train_data.columns:\n",
    "                le = LabelEncoder()\n",
    "                train_data[col] = le.fit_transform(train_data[col].astype(str))\n",
    "                test_data[col] = le.transform(test_data[col].astype(str)) \n",
    "        \n",
    "    if apply_pca:\n",
    "        # Perform PCA on scaled wavelength columns\n",
    "        pca = PCA(n_components=5)\n",
    "        wavelength_cols = train_data_combined.columns[54:]\n",
    "        \n",
    "        X_train_pca = pca.fit_transform(train_data_combined[wavelength_cols])\n",
    "        X_test_pca = pca.transform(test_data_combined[wavelength_cols])\n",
    "\n",
    "        # Combine PCA components with original data\n",
    "        X_train_combined = pd.concat([train_data_combined.iloc[:, :54].reset_index(drop=True), \n",
    "                                      pd.DataFrame(X_train_pca, columns=[f'PC{i+1}' for i in range(5)])], axis=1)\n",
    "        X_test_combined = pd.concat([test_data_combined.iloc[:, :54].reset_index(drop=True), \n",
    "                                     pd.DataFrame(X_test_pca, columns=[f'PC{i+1}' for i in range(5)])], axis=1)\n",
    "        \n",
    "        train_data_combined = X_train_combined\n",
    "        test_data_combined = X_test_combined\n",
    "        \n",
    "    \"\"\" if apply_random_forest:\n",
    "        # Apply Random Forest for feature selection\n",
    "        wavelength_cols = train_data_combined.columns[50:] \n",
    "        rf = RandomForestRegressor(n_estimators=300, random_state=42)\n",
    "        y_train = train_data['PURITY'].iloc[train_data_combined.index]\n",
    "        rf.fit(train_data_combined[wavelength_cols], y_train)\n",
    "        \n",
    "        # Select features based on importance\n",
    "        selector = SelectFromModel(rf, threshold=\"mean\", prefit=True)\n",
    "        train_data_combined = pd.DataFrame(selector.transform(train_data_combined[wavelength_cols]), \n",
    "                                           columns=train_data_combined[wavelength_cols].columns[selector.get_support()])\n",
    "        test_data_combined = pd.DataFrame(selector.transform(test_data_combined[wavelength_cols]), \n",
    "                                          columns=test_data_combined[wavelength_cols].columns[selector.get_support()])\n",
    "        print(f\"Shape after Random Forest feature selection: {train_data_combined.shape}\")  \"\"\"\n",
    "    \n",
    "    if apply_variance_threshold:\n",
    "        # Apply VarianceThreshold\n",
    "        selector = VarianceThreshold(threshold=0.05)\n",
    "        train_data_combined = pd.DataFrame(selector.fit_transform(train_data_combined), columns=train_data_combined.columns[selector.get_support(indices=True)])\n",
    "        test_data_combined = pd.DataFrame(selector.transform(test_data_combined), columns=test_data_combined.columns[selector.get_support(indices=True)])\n",
    "        print(f\"Shape after VarianceThreshold: {train_data_combined.shape}\")\n",
    "    \n",
    "    #Aussi tester Random forest à la placde de correlation matrix\n",
    "    if apply_correlation:\n",
    "    # Compute correlation matrix only for wavelength columns\n",
    "        #wavelength_cols = train_data_combined.columns[50:] \n",
    "        #correlation_matrix = train_data_combined[wavelength_cols].corr()\n",
    "        correlation_matrix = train_data_combined.corr()\n",
    "\n",
    "        # Visualize correlation matrix\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.heatmap(correlation_matrix, cmap='coolwarm', annot=False)\n",
    "        plt.title(\"Correlation Matrix for All Features\")\n",
    "        plt.show()\n",
    "\n",
    "        # Identify highly correlated features (e.g., |r| > 0.999)\n",
    "        threshold_high = 0.9999\n",
    "\n",
    "        print(f\"Number of features before removing highly correlated features: {train_data_combined.shape[1]}\")\n",
    "        \n",
    "        high_corr_pairs = [\n",
    "            (i, j)\n",
    "            for i in range(correlation_matrix.shape[0])\n",
    "            for j in range(i + 1, correlation_matrix.shape[1])\n",
    "            if abs(correlation_matrix.iloc[i, j]) > threshold_high\n",
    "        ]\n",
    "        \n",
    "        features_to_drop = set()\n",
    "        for i, j in high_corr_pairs:\n",
    "            features_to_drop.add(correlation_matrix.columns[j])  # Arbitrarily drop the second feature in the pair\n",
    "\n",
    "        # Remove the selected features\n",
    "        train_data_combined = train_data_combined.drop(columns=list(features_to_drop))\n",
    "        test_data_combined = test_data_combined.drop(columns=list(features_to_drop))\n",
    "        \n",
    "        #wavelength_cols = train_data_combined.columns[50:]\n",
    "        \n",
    "        print(f\"Number of features after removing highly correlated features: {train_data_combined.shape[1]}\")\n",
    "        \"\"\" print(\"Highly correlated features:\")\n",
    "        for i, j in high_corr_pairs:\n",
    "            print(f\"{correlation_matrix.columns[i]} and {correlation_matrix.columns[j]}: {correlation_matrix.iloc[i, j]}\") \"\"\"\n",
    "        \n",
    "    if apply_random_forest:\n",
    "        \n",
    "        wavelength_cols = train_data_combined.columns[50:]\n",
    "        X_train_rf = train_data_combined[wavelength_cols]\n",
    "        y_train_rf = train_data['PURITY'].iloc[train_data_combined.index].squeeze()\n",
    "\n",
    "        # Create and train random forest model\n",
    "        rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "        rf_model.fit(X_train_rf, y_train_rf)\n",
    "\n",
    "        # Perform feature selection using the specified threshold\n",
    "        sfm = SelectFromModel(rf_model, threshold=0.0048, prefit=True)\n",
    "        selected_features = sfm.get_support()\n",
    "        selected_feature_names = X_train_rf.columns[selected_features]\n",
    "\n",
    "        # Apply feature selection to train and test data\n",
    "        train_data_selected = train_data_combined[selected_feature_names]\n",
    "        test_data_selected = test_data_combined[selected_feature_names]\n",
    "\n",
    "        # Add back non-wavelength columns if needed\n",
    "        non_wavelength_cols = [col for col in train_data_combined.columns if col not in wavelength_cols]\n",
    "        train_data_combined = pd.concat([train_data_combined[non_wavelength_cols], train_data_selected], axis=1)\n",
    "        test_data_combined = pd.concat([test_data_combined[non_wavelength_cols], test_data_selected], axis=1)\n",
    "\n",
    "        print(f\"Selected {len(selected_feature_names)} features using Random Forest with threshold {0.0048}.\")\n",
    "\n",
    "        # Evaluate feature importances\n",
    "        feature_importances = rf_model.feature_importances_\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(range(len(feature_importances)), feature_importances)\n",
    "        plt.title(\"Feature Importances\")\n",
    "        plt.xlabel(\"Feature Index\")\n",
    "        plt.ylabel(\"Importance Score\")\n",
    "        plt.show()\n",
    "\n",
    "        \"\"\" # Test different thresholds\n",
    "        thresholds = [0.0025, 0.003, 0.0035, 0.004, 0.0045, 0.0048, 0.005, 0.0052]\n",
    "        cross_val_scores = []\n",
    "\n",
    "        for threshold in thresholds:\n",
    "            # Select features based on threshold\n",
    "            sfm = SelectFromModel(rf_model, threshold=threshold, prefit=True)\n",
    "            selected_features = sfm.get_support()\n",
    "            selected_feature_names = X_train_rf.columns[selected_features]\n",
    "\n",
    "            # Subset the dataset\n",
    "            X_train_selected = X_train_rf[selected_feature_names]\n",
    "\n",
    "            # Compute cross-validation scores\n",
    "            scores = cross_val_score(rf_model, X_train_selected, y_train_rf, cv=5, scoring='r2')\n",
    "            mean_score = scores.mean()\n",
    "            cross_val_scores.append(mean_score)\n",
    "\n",
    "            print(f\"Threshold: {threshold}\")\n",
    "            print(f\"Number of selected features: {len(selected_feature_names)}\")\n",
    "            print(f\"Cross-validated R^2 score: {mean_score:.4f}\")\n",
    "\n",
    "        # Plot cross-validated R^2 scores vs. thresholds\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(thresholds, cross_val_scores, marker='o')\n",
    "        plt.title(\"Cross-Validated R^2 Score vs. Threshold\")\n",
    "        plt.xlabel(\"Threshold\")\n",
    "        plt.ylabel(\"Mean R^2 Score\")\n",
    "        plt.grid()\n",
    "        plt.show() \"\"\"\n",
    "\n",
    "\n",
    "        \"\"\" # we obtain the names of the unwanted features\n",
    "        dropped_feature_names = X_train_rf.columns[feature_indices]\n",
    "        \n",
    "        train_data_combined = train_data_combined.drop(columns=dropped_feature_names)\n",
    "        test_data_combined = test_data_combined.drop(columns=dropped_feature_names) \"\"\"\n",
    "\n",
    "            \n",
    "    # Add sample_name column back to the combined DataFrames\n",
    "    train_data_combined.insert(0, 'sample_name', train_data_og['sample_name'])\n",
    "    test_data_combined.insert(0, 'sample_name', test_data_og['sample_name'])\n",
    "    y_train = train_data['PURITY'].iloc[train_data_combined.index]\n",
    "\n",
    "    print(f\"Shape of OG train data: {train_data_og.shape}\")\n",
    "    print(f\"Shape of OG test data: {test_data_og.shape}\")\n",
    "    print(f\"Shape of train data: {train_data_combined.shape}\")\n",
    "    print(f\"Shape of test data: {test_data_combined.shape}\")\n",
    "    print(f\"Shape of y_train: {y_train.shape}\")\n",
    "    \n",
    "    train_data_combined.to_csv('/Users/alicepriolet/Desktop/ML/epfl-bio-322-2024/train_data_combined.csv', index=False)\n",
    "    print('Submission file saved successfully.')\n",
    "    test_data_combined.to_csv('/Users/alicepriolet/Desktop/ML/epfl-bio-322-2024/test_data_combined.csv', index=False)\n",
    "    print('Submission file saved successfully.')\n",
    "            \n",
    "    return train_data_combined, test_data_combined, y_train\n",
    "\n",
    "def preprocessing_v0(apply_one_hot=False, apply_scaling=False, apply_pca=False, apply_correlation=False, apply_remove_outliers=False, apply_variance_threshold=False, apply_random_forest=False, enable_categorical=False):\n",
    "    train_data_og = pd.read_csv('/Users/alicepriolet/Desktop/ML/epfl-bio-322-2024/train_final.csv')\n",
    "    test_data_og = pd.read_csv('/Users/alicepriolet/Desktop/ML/epfl-bio-322-2024/test.csv')\n",
    "    train_data = train_data_og.copy()\n",
    "    test_data = test_data_og.copy()\n",
    "    train_data = train_data.drop(columns=['prod_substance'])\n",
    "    test_data = test_data.drop(columns=['prod_substance'])\n",
    "    \n",
    "    non_wavelength_cols = ['device_serial', 'substance_form_display', 'measure_type_display']\n",
    "    wavelength_cols = train_data.columns[5:]\n",
    "    \n",
    "    # Remove NaN values\n",
    "    train_data = train_data.dropna()\n",
    "    test_data = test_data.dropna()\n",
    "    \n",
    "    if apply_one_hot:\n",
    "        # One Hot encoding \n",
    "        encoder = OneHotEncoder(drop='first',sparse_output=False, handle_unknown='ignore')\n",
    "        X_train_encoded = encoder.fit_transform(train_data[non_wavelength_cols])\n",
    "        X_test_encoded = encoder.transform(test_data[non_wavelength_cols])\n",
    "        \n",
    "        # Convert encoded features to DataFrame\n",
    "        X_train_encoded_df = pd.DataFrame(X_train_encoded, columns=encoder.get_feature_names_out(non_wavelength_cols))\n",
    "        X_test_encoded_df = pd.DataFrame(X_test_encoded, columns=encoder.get_feature_names_out(non_wavelength_cols))\n",
    "        \n",
    "        train_data_combined = pd.concat([pd.DataFrame(X_train_encoded_df), train_data[wavelength_cols].reset_index(drop=True)], axis=1)\n",
    "        test_data_combined = pd.concat([pd.DataFrame(X_test_encoded_df), test_data[wavelength_cols].reset_index(drop=True)], axis=1)\n",
    "    else:\n",
    "        train_data_combined = train_data\n",
    "        test_data_combined = test_data  \n",
    "        \n",
    "    if apply_remove_outliers:\n",
    "        \n",
    "        non_outlier_indices = remove_outliers_mahalanobis(train_data_combined[wavelength_cols]).index\n",
    "        train_data_combined = train_data_combined.loc[non_outlier_indices].reset_index(drop=True)\n",
    "        print(f\"After Mahalanobis outlier removal, train data shape: {train_data_combined.shape}\") \n",
    "            \n",
    "    if apply_scaling:\n",
    "         # Standardisers\n",
    "        train_data_std = StandardScaler().fit(train_data_combined[wavelength_cols].values)\n",
    "\n",
    "        # Standardise the data\n",
    "        wavelength_train_scaled, wavelength_test_scaled = map(\n",
    "            lambda data, std_mach: std_mach.transform(data),\n",
    "            [\n",
    "                train_data_combined[wavelength_cols].values,\n",
    "                test_data_combined[wavelength_cols].values,\n",
    "            ],\n",
    "            [train_data_std, train_data_std],\n",
    "        )     \n",
    "        \n",
    "        train_data_combined[wavelength_cols] = pd.DataFrame(wavelength_train_scaled, columns=wavelength_cols)\n",
    "        test_data_combined[wavelength_cols] = pd.DataFrame(wavelength_test_scaled, columns=wavelength_cols)   \n",
    "        \n",
    "    if apply_pca:\n",
    "        # Perform PCA on scaled wavelength columns\n",
    "        pca = PCA(n_components=5)\n",
    "        wavelength_cols = train_data_combined.columns[54:]\n",
    "        \n",
    "        X_train_pca = pca.fit_transform(train_data_combined[wavelength_cols])\n",
    "        X_test_pca = pca.transform(test_data_combined[wavelength_cols])\n",
    "\n",
    "        # Combine PCA components with original data\n",
    "        X_train_combined = pd.concat([train_data_combined.iloc[:, :54].reset_index(drop=True), \n",
    "                                      pd.DataFrame(X_train_pca, columns=[f'PC{i+1}' for i in range(5)])], axis=1)\n",
    "        X_test_combined = pd.concat([test_data_combined.iloc[:, :54].reset_index(drop=True), \n",
    "                                     pd.DataFrame(X_test_pca, columns=[f'PC{i+1}' for i in range(5)])], axis=1)\n",
    "        \n",
    "        train_data_combined = X_train_combined\n",
    "        test_data_combined = X_test_combined\n",
    "        \n",
    "    \"\"\" if apply_random_forest:\n",
    "        # Apply Random Forest for feature selection\n",
    "        wavelength_cols = train_data_combined.columns[50:] \n",
    "        rf = RandomForestRegressor(n_estimators=300, random_state=42)\n",
    "        y_train = train_data['PURITY'].iloc[train_data_combined.index]\n",
    "        rf.fit(train_data_combined[wavelength_cols], y_train)\n",
    "        \n",
    "        # Select features based on importance\n",
    "        selector = SelectFromModel(rf, threshold=\"mean\", prefit=True)\n",
    "        train_data_combined = pd.DataFrame(selector.transform(train_data_combined[wavelength_cols]), \n",
    "                                           columns=train_data_combined[wavelength_cols].columns[selector.get_support()])\n",
    "        test_data_combined = pd.DataFrame(selector.transform(test_data_combined[wavelength_cols]), \n",
    "                                          columns=test_data_combined[wavelength_cols].columns[selector.get_support()])\n",
    "        print(f\"Shape after Random Forest feature selection: {train_data_combined.shape}\")  \"\"\"\n",
    "    \n",
    "    if apply_variance_threshold:\n",
    "        # Apply VarianceThreshold\n",
    "        selector = VarianceThreshold(threshold=0.05)\n",
    "        train_data_combined = pd.DataFrame(selector.fit_transform(train_data_combined), columns=train_data_combined.columns[selector.get_support(indices=True)])\n",
    "        test_data_combined = pd.DataFrame(selector.transform(test_data_combined), columns=test_data_combined.columns[selector.get_support(indices=True)])\n",
    "        print(f\"Shape after VarianceThreshold: {train_data_combined.shape}\")\n",
    "    \n",
    "    #Aussi tester Random forest à la placde de correlation matrix\n",
    "    if apply_correlation:\n",
    "    # Compute correlation matrix only for wavelength columns\n",
    "        #wavelength_cols = train_data_combined.columns[50:] \n",
    "        #correlation_matrix = train_data_combined[wavelength_cols].corr()\n",
    "        correlation_matrix = train_data_combined.corr()\n",
    "\n",
    "        # Visualize correlation matrix\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.heatmap(correlation_matrix, cmap='coolwarm', annot=False)\n",
    "        plt.title(\"Correlation Matrix for All Features\")\n",
    "        plt.show()\n",
    "\n",
    "        # Identify highly correlated features (e.g., |r| > 0.999)\n",
    "        threshold_high = 0.9999\n",
    "\n",
    "        print(f\"Number of features before removing highly correlated features: {train_data_combined.shape[1]}\")\n",
    "        \n",
    "        high_corr_pairs = [\n",
    "            (i, j)\n",
    "            for i in range(correlation_matrix.shape[0])\n",
    "            for j in range(i + 1, correlation_matrix.shape[1])\n",
    "            if abs(correlation_matrix.iloc[i, j]) > threshold_high\n",
    "        ]\n",
    "        \n",
    "        features_to_drop = set()\n",
    "        for i, j in high_corr_pairs:\n",
    "            features_to_drop.add(correlation_matrix.columns[j])  # Arbitrarily drop the second feature in the pair\n",
    "\n",
    "        # Remove the selected features\n",
    "        train_data_combined = train_data_combined.drop(columns=list(features_to_drop))\n",
    "        test_data_combined = test_data_combined.drop(columns=list(features_to_drop))\n",
    "        \n",
    "        #wavelength_cols = train_data_combined.columns[50:]\n",
    "        \n",
    "        print(f\"Number of features after removing highly correlated features: {train_data_combined.shape[1]}\")\n",
    "        \"\"\" print(\"Highly correlated features:\")\n",
    "        for i, j in high_corr_pairs:\n",
    "            print(f\"{correlation_matrix.columns[i]} and {correlation_matrix.columns[j]}: {correlation_matrix.iloc[i, j]}\") \"\"\"\n",
    "        \n",
    "    if apply_random_forest:\n",
    "        \n",
    "        wavelength_cols = train_data_combined.columns[50:]\n",
    "        X_train_rf = train_data_combined[wavelength_cols]\n",
    "        y_train_rf = train_data['PURITY'].iloc[train_data_combined.index].squeeze()\n",
    "\n",
    "        # Create and train random forest model\n",
    "        rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "        rf_model.fit(X_train_rf, y_train_rf)\n",
    "\n",
    "        # Perform feature selection using the specified threshold\n",
    "        sfm = SelectFromModel(rf_model, threshold=0.0048, prefit=True)\n",
    "        selected_features = sfm.get_support()\n",
    "        selected_feature_names = X_train_rf.columns[selected_features]\n",
    "\n",
    "        # Apply feature selection to train and test data\n",
    "        train_data_selected = train_data_combined[selected_feature_names]\n",
    "        test_data_selected = test_data_combined[selected_feature_names]\n",
    "\n",
    "        # Add back non-wavelength columns if needed\n",
    "        non_wavelength_cols = [col for col in train_data_combined.columns if col not in wavelength_cols]\n",
    "        train_data_combined = pd.concat([train_data_combined[non_wavelength_cols], train_data_selected], axis=1)\n",
    "        test_data_combined = pd.concat([test_data_combined[non_wavelength_cols], test_data_selected], axis=1)\n",
    "\n",
    "        print(f\"Selected {len(selected_feature_names)} features using Random Forest with threshold {0.0048}.\")\n",
    "\n",
    "        # Evaluate feature importances\n",
    "        feature_importances = rf_model.feature_importances_\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(range(len(feature_importances)), feature_importances)\n",
    "        plt.title(\"Feature Importances\")\n",
    "        plt.xlabel(\"Feature Index\")\n",
    "        plt.ylabel(\"Importance Score\")\n",
    "        plt.show()\n",
    "\n",
    "        \"\"\" # Test different thresholds\n",
    "        thresholds = [0.0025, 0.003, 0.0035, 0.004, 0.0045, 0.0048, 0.005, 0.0052]\n",
    "        cross_val_scores = []\n",
    "\n",
    "        for threshold in thresholds:\n",
    "            # Select features based on threshold\n",
    "            sfm = SelectFromModel(rf_model, threshold=threshold, prefit=True)\n",
    "            selected_features = sfm.get_support()\n",
    "            selected_feature_names = X_train_rf.columns[selected_features]\n",
    "\n",
    "            # Subset the dataset\n",
    "            X_train_selected = X_train_rf[selected_feature_names]\n",
    "\n",
    "            # Compute cross-validation scores\n",
    "            scores = cross_val_score(rf_model, X_train_selected, y_train_rf, cv=5, scoring='r2')\n",
    "            mean_score = scores.mean()\n",
    "            cross_val_scores.append(mean_score)\n",
    "\n",
    "            print(f\"Threshold: {threshold}\")\n",
    "            print(f\"Number of selected features: {len(selected_feature_names)}\")\n",
    "            print(f\"Cross-validated R^2 score: {mean_score:.4f}\")\n",
    "\n",
    "        # Plot cross-validated R^2 scores vs. thresholds\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(thresholds, cross_val_scores, marker='o')\n",
    "        plt.title(\"Cross-Validated R^2 Score vs. Threshold\")\n",
    "        plt.xlabel(\"Threshold\")\n",
    "        plt.ylabel(\"Mean R^2 Score\")\n",
    "        plt.grid()\n",
    "        plt.show() \"\"\"\n",
    "\n",
    "\n",
    "        \"\"\" # we obtain the names of the unwanted features\n",
    "        dropped_feature_names = X_train_rf.columns[feature_indices]\n",
    "        \n",
    "        train_data_combined = train_data_combined.drop(columns=dropped_feature_names)\n",
    "        test_data_combined = test_data_combined.drop(columns=dropped_feature_names) \"\"\"\n",
    "\n",
    "    if enable_categorical:\n",
    "        # Convertir les colonnes en type `category` pour XGBoost\n",
    "        for col in non_wavelength_cols:\n",
    "            if col in train_data.columns:\n",
    "                train_data[col] = train_data[col].astype('category')\n",
    "            if col in test_data.columns:\n",
    "                test_data[col] = test_data[col].astype('category')\n",
    "    else:\n",
    "        # Encodage des colonnes catégoriques avec LabelEncoder\n",
    "        for col in non_wavelength_cols:\n",
    "            if col in train_data.columns:\n",
    "                le = LabelEncoder()\n",
    "                train_data[col] = le.fit_transform(train_data[col].astype(str))\n",
    "                test_data[col] = le.transform(test_data[col].astype(str)) \n",
    "        \n",
    "            \n",
    "    # Add sample_name column back to the combined DataFrames\n",
    "    #train_data_combined.insert(0, 'sample_name', train_data_og['sample_name'])\n",
    "    #test_data_combined.insert(0, 'sample_name', test_data_og['sample_name'])\n",
    "    y_train = train_data['PURITY'].iloc[train_data_combined.index]\n",
    "\n",
    "    print(f\"Shape of OG train data: {train_data_og.shape}\")\n",
    "    print(f\"Shape of OG test data: {test_data_og.shape}\")\n",
    "    print(f\"Shape of train data: {train_data_combined.shape}\")\n",
    "    print(f\"Shape of test data: {test_data_combined.shape}\")\n",
    "    print(f\"Shape of y_train: {y_train.shape}\")\n",
    "    \n",
    "    train_data_combined.to_csv('/Users/alicepriolet/Desktop/ML/epfl-bio-322-2024/train_data_combined.csv', index=False)\n",
    "    print('Submission file saved successfully.')\n",
    "    test_data_combined.to_csv('/Users/alicepriolet/Desktop/ML/epfl-bio-322-2024/test_data_combined.csv', index=False)\n",
    "    print('Submission file saved successfully.')\n",
    "            \n",
    "    return train_data_combined, test_data_combined, y_train\n",
    "  \n",
    "def submission_file(y_test_predicted):\n",
    "    submission_reduced = pd.DataFrame({\n",
    "        'ID': range(1, len(y_test_predicted) + 1),\n",
    "        'PURITY': y_test_predicted\n",
    "    })\n",
    "    return submission_reduced\n",
    "\n",
    "def preprocessing_vX(apply_one_hot=False, apply_scaling=True, enable_categorical=False):\n",
    "    # Charger les fichiers train et test\n",
    "    train_data = pd.read_csv('/Users/alicepriolet/Desktop/ML/epfl-bio-322-2024/train_final.csv')\n",
    "    test_data = pd.read_csv('/Users/alicepriolet/Desktop/ML/epfl-bio-322-2024/test.csv')\n",
    "    \n",
    "    # Supprimer les colonnes inutiles\n",
    "    non_wavelength_cols = ['sample_name', 'device_serial', 'substance_form_display', 'measure_type_display']\n",
    "    train_data = train_data.drop(columns=['prod_substance'], errors='ignore')\n",
    "    test_data = test_data.drop(columns=['prod_substance'], errors='ignore')\n",
    "    \n",
    "    # Identifier les colonnes des longueurs d'onde\n",
    "    wavelength_cols = train_data.columns[6:]\n",
    "\n",
    "    # Gérer les colonnes catégoriques\n",
    "    if enable_categorical:\n",
    "        # Convertir les colonnes en type `category` pour XGBoost\n",
    "        for col in non_wavelength_cols:\n",
    "            if col in train_data.columns:\n",
    "                train_data[col] = train_data[col].astype('category')\n",
    "            if col in test_data.columns:\n",
    "                test_data[col] = test_data[col].astype('category')\n",
    "    else:\n",
    "        # Encodage des colonnes catégoriques avec LabelEncoder\n",
    "        for col in non_wavelength_cols:\n",
    "            if col in train_data.columns:\n",
    "                le = LabelEncoder()\n",
    "                train_data[col] = le.fit_transform(train_data[col].astype(str))\n",
    "                test_data[col] = le.transform(test_data[col].astype(str))\n",
    "\n",
    "    # Normalisation des colonnes numériques (longueurs d'onde)\n",
    "    if apply_scaling:\n",
    "        scaler = StandardScaler()\n",
    "        train_data[wavelength_cols] = scaler.fit_transform(train_data[wavelength_cols])\n",
    "        test_data[wavelength_cols] = scaler.transform(test_data[wavelength_cols])\n",
    "\n",
    "    y_train = train_data['PURITY']\n",
    "    train_data = train_data.drop(columns=['PURITY'])\n",
    "\n",
    "    return train_data, test_data, y_train\n",
    "\n",
    "def xg_boost():\n",
    "    \n",
    "    X_train, X_test, y_train = preprocessing_vX(apply_one_hot=False, apply_scaling=True, enable_categorical = True)\n",
    "\n",
    "    #X_train = X_train.drop(columns=['sample_name'])\n",
    "    #X_test = X_test.drop(columns=['sample_name'])\n",
    "    target_column = 'PURITY'\n",
    "\n",
    "\n",
    "    param_grid = {\n",
    "        'colsample_bytree': np.linspace(0.5, 1, 5),\n",
    "        'subsample': np.linspace(0.5, 1, 5),\n",
    "        'max_depth': np.arange(2, 7, 1)\n",
    "    }\n",
    "\n",
    "\n",
    "    model = XGBRegressor(n_estimators=200, max_depth=10, learning_rate=0.1, random_state=42,enable_categorical=True)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Create submission DataFrame\n",
    "    submission = submission_file(y_pred)\n",
    "    \n",
    "    # Save submission to CSV\n",
    "    submission.to_csv('/Users/alicepriolet/Desktop/ML/epfl-bio-322-2024/sample_submission_XGB_substances.csv', index=False)\n",
    "    print('Submission file saved successfully.')   \n",
    "\n",
    "\n",
    "def linear_regression(feature_importance=False, apply_y_transformation=False):\n",
    "    X_train, X_test, y_train = preprocessing_vX(apply_one_hot=True, apply_scaling=True)\n",
    "    X_train = X_train.drop(columns=['sample_name'])\n",
    "    X_test = X_test.drop(columns=['sample_name'])\n",
    "    \n",
    "    if feature_importance:\n",
    "        X_train, X_test = calculate_feature_importance(X_train, y_train, X_test)\n",
    "        \n",
    "    if apply_y_transformation:\n",
    "        y_train = apply_log_transformation(y_train)\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_train_pred = model.predict(X_train)\n",
    "    mse = mean_squared_error(y_train, y_train_pred)\n",
    "    print('Training MSE:', mse)\n",
    "    \n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "    print('CV MSE:', -cv_scores.mean())\n",
    "    \n",
    "    # Predict on test data\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Create submission DataFrame\n",
    "    submission = submission_file(y_test_pred)\n",
    "    \n",
    "    # Save submission to CSV\n",
    "    submission.to_csv('/Users/maelysclerget/Desktop/ML/bio322_project/epfl-bio-322-2024/sample_submission_LR.csv', index=False)\n",
    "    print('Submission file saved successfully.')\n",
    "\n",
    "def main():\n",
    "    #add_substance_form_display()\n",
    "    #rename_and_modify_substance_column()\n",
    "    #device_serial_proba()\n",
    "    #add_device_serial()\n",
    "    #add_sample_name()\n",
    "    #merge_csv_files()\n",
    "    #preprocessing_v1()\n",
    "    #xg_boost()\n",
    "    linear_regression()\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main() \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLCourse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
