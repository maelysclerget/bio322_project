{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA files saved successfully.\n",
      "Training MSE: 55.5481669002354\n",
      "Cross-Validation MSE: 1.2504677040306736e+22\n",
      "Submission file saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.stats import zscore\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def preprocessing_v1():\n",
    "    \n",
    "    train_data_og = pd.read_csv('/Users/maelysclerget/Desktop/ML/bio322_project/epfl-bio-322-2024/train.csv')\n",
    "    test_data_og = pd.read_csv('/Users/maelysclerget/Desktop/ML/bio322_project/epfl-bio-322-2024/test.csv')\n",
    "    train_data = train_data_og.copy()\n",
    "    test_data = test_data_og.copy()\n",
    "    train_data = train_data.drop(columns=['prod_substance'])\n",
    "    test_data = test_data.drop(columns=['prod_substance'])\n",
    "    \n",
    "    non_wavelength_cols = ['device_serial','substance_form_display','measure_type_display']\n",
    "    wavelength_cols = train_data.columns[6:]\n",
    "    \n",
    "    #One Hot encoding \n",
    "    encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "    X_train_encoded = encoder.fit_transform(train_data[non_wavelength_cols])\n",
    "    X_test_encoded = encoder.transform(test_data[non_wavelength_cols])\n",
    "    \n",
    "    # Convert encoded features to DataFrame\n",
    "    X_train_encoded_df = pd.DataFrame(X_train_encoded, columns=encoder.get_feature_names_out(non_wavelength_cols))\n",
    "    X_test_encoded_df = pd.DataFrame(X_test_encoded, columns=encoder.get_feature_names_out(non_wavelength_cols))\n",
    "    \n",
    "    train_data_combined = pd.concat([pd.DataFrame(X_train_encoded_df), train_data[wavelength_cols].reset_index(drop=True)], axis=1)\n",
    "    test_data_combined = pd.concat([pd.DataFrame(X_test_encoded_df), test_data[wavelength_cols].reset_index(drop=True)], axis=1)\n",
    "    \n",
    "    # Add sample_name column back to the combined DataFrames\n",
    "    train_data_combined.insert(0, 'sample_name', train_data_og['sample_name'])\n",
    "    test_data_combined.insert(0, 'sample_name', test_data_og['sample_name'])\n",
    "    \n",
    "    #Remove NaN values\n",
    "    train_data_combined = train_data_combined.dropna()\n",
    "    test_data_combined = test_data_combined.dropna()\n",
    "    \n",
    "    y_train = train_data['PURITY'].iloc[train_data_combined.index]\n",
    "    \n",
    "    #Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    wavelength_train_scaled = scaler.fit_transform(train_data_combined[wavelength_cols])\n",
    "    wavelength_test_scaled = scaler.transform(test_data_combined[wavelength_cols])\n",
    "    \n",
    "    train_data_combined[wavelength_cols] = wavelength_train_scaled\n",
    "    test_data_combined[wavelength_cols] = wavelength_test_scaled\n",
    "    \n",
    "    # Apply Savitzky-Golay filter\n",
    "    #window_length = 7  # Choose an appropriate window length\n",
    "    #polyorder = 3      # Choose an appropriate polynomial order\n",
    "    #train_data_combined[wavelength_cols] = savgol_filter(train_data_combined[wavelength_cols], window_length, polyorder, axis=0)\n",
    "    #test_data_combined[wavelength_cols] = savgol_filter(test_data_combined[wavelength_cols], window_length, polyorder, axis=0)\n",
    "    \n",
    "    #outliers\n",
    "    outliers_index = (np.abs(wavelength_train_scaled) > 3).any(axis=1)\n",
    "    train_data_combined = train_data_combined[~outliers_index] #exclude outliers\n",
    "    y_train = y_train[~outliers_index]\n",
    "    \n",
    "    X_train_final = train_data_combined.reset_index(drop=True)\n",
    "    X_test_final = test_data_combined.reset_index(drop=True)\n",
    "    Y_train_final = y_train.reset_index(drop=True)\n",
    "    \n",
    "    X_train_final.to_csv('/Users/maelysclerget/Desktop/ML/bio322_project/epfl-bio-322-2024/preprocessed_train_data.csv', index=False)\n",
    "    X_test_final.to_csv('/Users/maelysclerget/Desktop/ML/bio322_project/epfl-bio-322-2024/preprocessed_test_data.csv', index=False)\n",
    "    \n",
    "    return X_train_final, X_test_final, Y_train_final\n",
    "\n",
    "def correlation():\n",
    "    \n",
    "    X_train, X_test, y_train = preprocessing_v1()\n",
    "     \n",
    "    wavelength_cols = X_train.columns[54:]\n",
    "    \n",
    "    # Compute correlation matrix only for wavelength columns\n",
    "    correlation_matrix = X_train[wavelength_cols].corr()\n",
    "\n",
    "    # Visualize correlation matrix\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(correlation_matrix, cmap='coolwarm', annot=False)\n",
    "    plt.title(\"Correlation Matrix for Wavelength Features\")\n",
    "    plt.show()\n",
    "\n",
    "    # Identify highly correlated features (e.g., |r| > 0.95)\n",
    "    threshold_high = 0.999\n",
    "    threshold_low = 0.2\n",
    "\n",
    "    high_corr_pairs = [\n",
    "        (i, j)\n",
    "        for i in range(correlation_matrix.shape[0])\n",
    "        for j in range(i + 1, correlation_matrix.shape[1])\n",
    "        if abs(correlation_matrix.iloc[i, j]) > threshold_high\n",
    "    ]\n",
    "    \n",
    "    features_to_drop = set()\n",
    "    for i, j in high_corr_pairs:\n",
    "        features_to_drop.add(wavelength_cols[j])  # Arbitrarily drop the second feature in the pair\n",
    "\n",
    "    # Remove the selected features\n",
    "    X_train_reduced = X_train.drop(columns=list(features_to_drop))\n",
    "    X_test_reduced = X_test.drop(columns=list(features_to_drop))\n",
    "    \n",
    "    low_corr_pairs = [\n",
    "        (i, j)\n",
    "        for i in range(correlation_matrix.shape[0])\n",
    "        for j in range(i + 1, correlation_matrix.shape[1])\n",
    "        if abs(correlation_matrix.iloc[i, j]) < threshold_low\n",
    "    ]\n",
    "\n",
    "    print(\"Highly correlated features:\")\n",
    "    for i, j in high_corr_pairs:\n",
    "        print(f\"{wavelength_cols[i]} and {wavelength_cols[j]}: {correlation_matrix.iloc[i, j]}\")\n",
    "    print(\"Low correlated features:\")\n",
    "    for i, j in low_corr_pairs:\n",
    "        print(f\"{wavelength_cols[i]} and {wavelength_cols[j]}: {correlation_matrix.iloc[i, j]}\")\n",
    "        \n",
    "    return X_train_reduced, X_test_reduced\n",
    "\n",
    "def linear_regression_drop_features():\n",
    "    \n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    \n",
    "    X_train, X_test = correlation()\n",
    "    _, _, y_train = preprocessing_v1()\n",
    "    X_train = X_train.drop(columns=['sample_name'])\n",
    "    X_test = X_test.drop(columns=['sample_name'])\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_train_pred = model.predict(X_train)\n",
    "    mse = mean_squared_error(y_train, y_train_pred)\n",
    "    print('Training MSE:', mse)\n",
    "    \n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "    print('CV MSE:', -cv_scores.mean())\n",
    "    \n",
    "    # Predict on test data\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Create submission DataFrame\n",
    "    submission = submission_file(y_test_pred)\n",
    "    \n",
    "    # Save submission to CSV\n",
    "    submission.to_csv('/Users/maelysclerget/Desktop/ML/bio322_project/epfl-bio-322-2024/sample_submission_LR_drop_features.csv', index=False)\n",
    "    print('Submission file saved successfully.')   \n",
    "\n",
    "def pca():\n",
    "    \n",
    "    from sklearn.decomposition import PCA\n",
    "    import numpy as np\n",
    "    \n",
    "    X_train, X_test, y_train = preprocessing_v1()\n",
    "    wavelength_cols = X_train.columns[54:]\n",
    "\n",
    "    # Perform PCA on scaled wavelength columns\n",
    "    pca = PCA(n_components=5)\n",
    "    X_train_pca = pca.fit_transform(X_train[wavelength_cols])\n",
    "    X_test_pca = pca.transform(X_test[wavelength_cols])\n",
    "\n",
    "    # # Analyze cumulative variance explained\n",
    "    # explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "    # # Plot cumulative variance explained\n",
    "    # plt.figure(figsize=(8, 6))\n",
    "    # plt.plot(range(1, len(explained_variance) + 1), explained_variance, marker='o', linestyle='--')\n",
    "    # plt.axhline(y=0.999, color='r', linestyle='-')\n",
    "    # plt.title(\"Cumulative Explained Variance\")\n",
    "    # plt.xlabel(\"Number of Principal Components\")\n",
    "    # plt.ylabel(\"Cumulative Variance Explained\")\n",
    "    # plt.grid()\n",
    "    # plt.show()\n",
    "\n",
    "    # # Number of components explaining >95% variance\n",
    "    # n_components_999 = np.argmax(explained_variance >= 0.999) + 1\n",
    "    # print(f\"Number of components explaining >99.9% variance: {n_components_999}\")\n",
    "\n",
    "    X_train_combined = pd.concat([X_train.iloc[:, :54].reset_index(drop=True), \n",
    "                                  pd.DataFrame(X_train_pca, columns=[f'PC{i+1}' for i in range(5)])], axis=1)\n",
    "    X_test_combined = pd.concat([X_test.iloc[:, :54].reset_index(drop=True), \n",
    "                                 pd.DataFrame(X_test_pca, columns=[f'PC{i+1}' for i in range(5)])], axis=1)\n",
    "        \n",
    "    X_train_combined.to_csv('/Users/maelysclerget/Desktop/ML/bio322_project/epfl-bio-322-2024/PCA_train.csv', index=False)\n",
    "    X_test_combined.to_csv('/Users/maelysclerget/Desktop/ML/bio322_project/epfl-bio-322-2024/PCA_test.csv', index=False)\n",
    "\n",
    "    print('PCA files saved successfully.')\n",
    "    \n",
    "    return X_train_combined, X_test_combined\n",
    "\n",
    "def linear_regression():\n",
    "    \n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    \n",
    "    X_train, X_test, y_train = preprocessing_v1()\n",
    "    X_train = X_train.drop(columns=['sample_name'])\n",
    "    X_test = X_test.drop(columns=['sample_name'])\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on training data\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    mse = mean_squared_error(y_train, y_train_pred)\n",
    "    print('Training MSE:', mse)\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "    print('CV MSE:', -cv_scores.mean())\n",
    "    \n",
    "    # Predict on test data\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Create submission DataFrame\n",
    "    submission = submission_file(y_test_pred)\n",
    "    \n",
    "    # Save submission to CSV\n",
    "    submission.to_csv('/Users/maelysclerget/Desktop/ML/bio322_project/epfl-bio-322-2024/sample_submission_LR.csv', index=False)\n",
    "    print('Submission file saved successfully.')\n",
    "    \n",
    "    # Print feature importance\n",
    "    feature_importance = pd.Series(model.coef_, index=X_train.columns)\n",
    "    feature_importance = feature_importance.abs().sort_values(ascending=False)\n",
    "    wavelength_feature_importance_df = feature_importance.reset_index()\n",
    "    wavelength_feature_importance_df.columns = ['Feature', 'Importance']\n",
    "    wavelength_feature_importance_df.to_csv('/Users/maelysclerget/Desktop/ML/bio322_project/epfl-bio-322-2024/feature_importance_LR1.csv', index=False)\n",
    "    print('Feature Importance saved successfully.')\n",
    "    \n",
    "    # Calculate stats threshold\n",
    "    threshold1 = feature_importance.quantile(0.25)\n",
    "    threshold2 = feature_importance.mean()\n",
    "    threshold3 = feature_importance.median()\n",
    "    \n",
    "    # Identify low-importance features\n",
    "    low_importance_features = feature_importance[feature_importance < threshold1].index\n",
    "    print(f'Low importance features: {low_importance_features}')\n",
    "    \n",
    "    # Remove low-importance features and retrain the model\n",
    "    X_train_reduced = X_train.drop(columns=low_importance_features)\n",
    "    X_test_reduced = X_test.drop(columns=low_importance_features)\n",
    "    \n",
    "    model_reduced = LinearRegression()\n",
    "    model_reduced.fit(X_train_reduced, y_train)\n",
    "    \n",
    "    # Predict on training data with reduced features\n",
    "    y_train_pred_reduced = model_reduced.predict(X_train_reduced)\n",
    "    mse_reduced = mean_squared_error(y_train, y_train_pred_reduced)\n",
    "    print('Training MSE with reduced features:', mse_reduced)\n",
    "    \n",
    "    # Cross-validation with reduced features\n",
    "    cv_scores_reduced = cross_val_score(model_reduced, X_train_reduced, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "    print('CV MSE with reduced features:', -cv_scores_reduced.mean())\n",
    "    \n",
    "    # Predict on test data with reduced features\n",
    "    y_test_pred_reduced = model_reduced.predict(X_test_reduced)\n",
    "    \n",
    "    # Create submission DataFrame with reduced features\n",
    "    submission_reduced = submission_file(y_test_pred_reduced)\n",
    "    \n",
    "    # Save submission with reduced features to CSV\n",
    "    submission_reduced.to_csv('/Users/maelysclerget/Desktop/ML/bio322_project/epfl-bio-322-2024/sample_submission_LR_reduced.csv', index=False)\n",
    "    print('Submission file with reduced features saved successfully.')\n",
    "    \n",
    "def linear_regression_pca():\n",
    "    \n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "\n",
    "    # Load PCA-transformed training data\n",
    "    X_train_pca, X_test_pca = pca()\n",
    "    \n",
    "    # Load the original test data from preprocessing\n",
    "    X_train, X_test, y_train = preprocessing_v1()\n",
    "    X_test_pca = X_test_pca.drop(columns=['sample_name'])  # Drop sample_name for modeling\n",
    "    X_train_pca = X_train_pca.drop(columns=['sample_name'])\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_pca, y_train)\n",
    "\n",
    "    # Predict on the training data\n",
    "    y_train_pred = model.predict(X_train_pca)\n",
    "    mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "    print(f'Training MSE: {mse_train}')\n",
    "\n",
    "    # Cross-validation for validation error\n",
    "    cv_scores = cross_val_score(model, X_train_pca, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "    cv_mse = -cv_scores.mean()\n",
    "    print(f'Cross-Validation MSE: {cv_mse}')\n",
    "\n",
    "    # Predict on test data\n",
    "    y_test_pred = model.predict(X_test_pca)\n",
    "\n",
    "    # Format test predictions for submission\n",
    "    submission = submission_file(y_test_pred)\n",
    "    \n",
    "    # Save predictions to CSV\n",
    "    submission.to_csv('/Users/maelysclerget/Desktop/ML/bio322_project/epfl-bio-322-2024/sample_submission_LR_PCA.csv', index=False)\n",
    "    print('Submission file saved successfully.')\n",
    "\n",
    "# def polynomial_regression():\n",
    "    \n",
    "# def knn():\n",
    "\n",
    "# PURITY log transforom\n",
    "    \n",
    "def submission_file(y_test_predicted):\n",
    "    submission_reduced = pd.DataFrame({\n",
    "        'ID': range(1, len(y_test_predicted) + 1),\n",
    "        'PURITY': y_test_predicted\n",
    "    })\n",
    "    return submission_reduced\n",
    "    \n",
    "def main():\n",
    "    #linear_regression()\n",
    "    #correlation()\n",
    "    #pca()\n",
    "    linear_regression_pca()\n",
    "    #linear_regression_drop_features()\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLCourse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
