{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maelysclerget/anaconda3/envs/MLCourse/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [0, 1] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "/Users/maelysclerget/anaconda3/envs/MLCourse/lib/python3.10/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but SelectFromModel was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/maelysclerget/anaconda3/envs/MLCourse/lib/python3.10/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but SelectFromModel was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after Random Forest feature selection: (1264, 23)\n",
      "Shape of train data: (1264, 24)\n",
      "Shape of test data: (608, 24)\n",
      "Training MSE: 3.2923181846130896\n",
      "CV MSE: 25.608949129977344\n",
      "Submission file saved successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# def knn():\n",
    "# PURITY log transform  \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "from sklearn.linear_model import OrthogonalMatchingPursuit\n",
    "\n",
    "\n",
    "def preprocessing_v1(apply_one_hot=False, apply_scaling=False, apply_pca=False, apply_correlation=False, apply_remove_outliers=False, apply_variance_threshold=False, apply_random_forest=False):\n",
    "    train_data_og = pd.read_csv('/Users/maelysclerget/Desktop/ML/bio322_project/epfl-bio-322-2024/train.csv')\n",
    "    test_data_og = pd.read_csv('/Users/maelysclerget/Desktop/ML/bio322_project/epfl-bio-322-2024/test.csv')\n",
    "    train_data = train_data_og.copy()\n",
    "    test_data = test_data_og.copy()\n",
    "    train_data = train_data.drop(columns=['prod_substance'])\n",
    "    test_data = test_data.drop(columns=['prod_substance'])\n",
    "    \n",
    "    non_wavelength_cols = ['device_serial', 'substance_form_display', 'measure_type_display']\n",
    "    wavelength_cols = train_data.columns[6:]\n",
    "    \n",
    "    if apply_one_hot:\n",
    "        # One Hot encoding \n",
    "        encoder = OneHotEncoder(drop='first',sparse_output=False, handle_unknown='ignore')\n",
    "        X_train_encoded = encoder.fit_transform(train_data[non_wavelength_cols])\n",
    "        X_test_encoded = encoder.transform(test_data[non_wavelength_cols])\n",
    "        \n",
    "        # Convert encoded features to DataFrame\n",
    "        X_train_encoded_df = pd.DataFrame(X_train_encoded, columns=encoder.get_feature_names_out(non_wavelength_cols))\n",
    "        X_test_encoded_df = pd.DataFrame(X_test_encoded, columns=encoder.get_feature_names_out(non_wavelength_cols))\n",
    "        \n",
    "        train_data_combined = pd.concat([pd.DataFrame(X_train_encoded_df), train_data[wavelength_cols].reset_index(drop=True)], axis=1)\n",
    "        test_data_combined = pd.concat([pd.DataFrame(X_test_encoded_df), test_data[wavelength_cols].reset_index(drop=True)], axis=1)\n",
    "    else:\n",
    "        train_data_combined = train_data\n",
    "        test_data_combined = test_data\n",
    "    \n",
    "    # Remove NaN values\n",
    "    train_data_combined = train_data_combined.dropna()\n",
    "    test_data_combined = test_data_combined.dropna()\n",
    "    \n",
    "    y_train = train_data['PURITY'].iloc[train_data_combined.index]\n",
    "    \n",
    "    #NOTE: toujours mettre scaling avec remove_outliers\n",
    "    \n",
    "    if apply_scaling:\n",
    "        # Standardize the data\n",
    "        scaler = StandardScaler()\n",
    "        wavelength_train_scaled = scaler.fit_transform(train_data_combined[wavelength_cols])\n",
    "        wavelength_test_scaled = scaler.transform(test_data_combined[wavelength_cols])\n",
    "        \n",
    "        train_data_combined[wavelength_cols] = wavelength_train_scaled\n",
    "        test_data_combined[wavelength_cols] = wavelength_test_scaled\n",
    "    \n",
    "    if apply_scaling and apply_remove_outliers:\n",
    "        # Remove outliers\n",
    "        outliers_index = (np.abs(wavelength_train_scaled) > 3).any(axis=1)\n",
    "        train_data_combined = train_data_combined[~outliers_index]\n",
    "        y_train = y_train[~outliers_index]\n",
    "        \n",
    "        train_data_combined = train_data_combined.reset_index(drop=True)\n",
    "        test_data_combined = test_data_combined.reset_index(drop=True)\n",
    "        y_train = y_train.reset_index(drop=True)   \n",
    "        \n",
    "    if apply_pca:\n",
    "        # Perform PCA on scaled wavelength columns\n",
    "        pca = PCA(n_components=5)\n",
    "        wavelength_cols = train_data_combined.columns[54:]\n",
    "        \n",
    "        X_train_pca = pca.fit_transform(train_data_combined[wavelength_cols])\n",
    "        X_test_pca = pca.transform(test_data_combined[wavelength_cols])\n",
    "\n",
    "        # Combine PCA components with original data\n",
    "        X_train_combined = pd.concat([train_data_combined.iloc[:, :54].reset_index(drop=True), \n",
    "                                      pd.DataFrame(X_train_pca, columns=[f'PC{i+1}' for i in range(5)])], axis=1)\n",
    "        X_test_combined = pd.concat([test_data_combined.iloc[:, :54].reset_index(drop=True), \n",
    "                                     pd.DataFrame(X_test_pca, columns=[f'PC{i+1}' for i in range(5)])], axis=1)\n",
    "        \n",
    "        train_data_combined = X_train_combined\n",
    "        test_data_combined = X_test_combined\n",
    "        \n",
    "    if apply_random_forest:\n",
    "        # Apply Random Forest for feature selection\n",
    "        wavelength_cols = train_data_combined.columns[54:] \n",
    "        rf = RandomForestRegressor(n_estimators=300, random_state=42)\n",
    "        rf.fit(train_data_combined[wavelength_cols], y_train)\n",
    "        \n",
    "        # Select features based on importance\n",
    "        selector = SelectFromModel(rf, threshold=\"mean\", prefit=True)\n",
    "        train_data_combined = pd.DataFrame(selector.transform(train_data_combined[wavelength_cols]), \n",
    "                                           columns=train_data_combined[wavelength_cols].columns[selector.get_support()])\n",
    "        test_data_combined = pd.DataFrame(selector.transform(test_data_combined[wavelength_cols]), \n",
    "                                          columns=test_data_combined[wavelength_cols].columns[selector.get_support()])\n",
    "        print(f\"Shape after Random Forest feature selection: {train_data_combined.shape}\") \n",
    "    \n",
    "    if apply_variance_threshold:\n",
    "        # Apply VarianceThreshold\n",
    "        selector = VarianceThreshold(threshold=0.05)\n",
    "        train_data_combined = pd.DataFrame(selector.fit_transform(train_data_combined), columns=train_data_combined.columns[selector.get_support(indices=True)])\n",
    "        test_data_combined = pd.DataFrame(selector.transform(test_data_combined), columns=test_data_combined.columns[selector.get_support(indices=True)])\n",
    "        print(f\"Shape after VarianceThreshold: {train_data_combined.shape}\")\n",
    "        \n",
    "    \n",
    "    #Aussi tester Random forest à la placde de correlation matrix\n",
    "    if apply_correlation:\n",
    "    # Compute correlation matrix only for wavelength columns\n",
    "        wavelength_cols = train_data_combined.columns[54:] \n",
    "        correlation_matrix = train_data_combined[wavelength_cols].corr()\n",
    "\n",
    "        # Visualize correlation matrix\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.heatmap(correlation_matrix, cmap='coolwarm', annot=False)\n",
    "        plt.title(\"Correlation Matrix for Wavelength Features\")\n",
    "        plt.show()\n",
    "\n",
    "        # Identify highly correlated features (e.g., |r| > 0.999)\n",
    "        threshold_high = 0.999\n",
    "        threshold_low = 0.2\n",
    "\n",
    "        print(f\"Number of features before removing highly correlated features: {train_data_combined.shape[1]}\")\n",
    "        \n",
    "        high_corr_pairs = [\n",
    "            (i, j)\n",
    "            for i in range(correlation_matrix.shape[0])\n",
    "            for j in range(i + 1, correlation_matrix.shape[1])\n",
    "            if abs(correlation_matrix.iloc[i, j]) > threshold_high\n",
    "        ]\n",
    "        \n",
    "        features_to_drop = set()\n",
    "        for i, j in high_corr_pairs:\n",
    "            features_to_drop.add(wavelength_cols[j])  # Arbitrarily drop the second feature in the pair\n",
    "\n",
    "        # Remove the selected features\n",
    "        train_data_combined = train_data_combined.drop(columns=list(features_to_drop))\n",
    "        test_data_combined = test_data_combined.drop(columns=list(features_to_drop))\n",
    "        \n",
    "        print(f\"Number of features after removing highly correlated features: {train_data_combined.shape[1]}\")\n",
    "        \n",
    "        low_corr_pairs = [\n",
    "            (i, j)\n",
    "            for i in range(correlation_matrix.shape[0])\n",
    "            for j in range(i + 1, correlation_matrix.shape[1])\n",
    "            if abs(correlation_matrix.iloc[i, j]) < threshold_low\n",
    "        ]\n",
    "\n",
    "        print(\"Highly correlated features:\")\n",
    "        for i, j in high_corr_pairs:\n",
    "            print(f\"{wavelength_cols[i]} and {wavelength_cols[j]}: {correlation_matrix.iloc[i, j]}\")\n",
    "        print(\"Low correlated features:\")\n",
    "        for i, j in low_corr_pairs:\n",
    "            print(f\"{wavelength_cols[i]} and {wavelength_cols[j]}: {correlation_matrix.iloc[i, j]}\")\n",
    "            \n",
    "    # Add sample_name column back to the combined DataFrames\n",
    "    train_data_combined.insert(0, 'sample_name', train_data_og['sample_name'])\n",
    "    test_data_combined.insert(0, 'sample_name', test_data_og['sample_name'])\n",
    "    print(f\"Shape of train data: {train_data_combined.shape}\")\n",
    "    print(f\"Shape of test data: {test_data_combined.shape}\")\n",
    "            \n",
    "    return train_data_combined, test_data_combined, y_train\n",
    "\n",
    "def best_param_RF():\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200, 300],\n",
    "        'random_state': [42]\n",
    "    }\n",
    "\n",
    "    # Initialize the RandomForestRegressor\n",
    "    rf = RandomForestRegressor()\n",
    "\n",
    "    # Initialize GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "    # Preprocess the data\n",
    "    X_train, X_test, y_train = preprocessing_v1(apply_one_hot=True, apply_scaling=True, apply_remove_outliers=True)\n",
    "    X_train = X_train.drop(columns=['sample_name'])\n",
    "    X_test = X_test.drop(columns=['sample_name'])\n",
    "\n",
    "    # Fit GridSearchCV\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best model\n",
    "    best_rf = grid_search.best_estimator_\n",
    "    print(\"Best parameters:\", grid_search.best_params_)\n",
    "    return best_rf\n",
    "\n",
    "    \n",
    "# Alice tu peux refaire ca en plus beau please (c'est pour voir quel treshold  mettre pour la variance) \n",
    "def plot_variance():\n",
    "    X_train, X_test, y_train = preprocessing_v1(apply_one_hot=True, apply_scaling=True, apply_remove_outliers=True)\n",
    "    \n",
    "    # Select only numerical columns\n",
    "    numerical_cols = X_train.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    # Calculate variance for each numerical feature\n",
    "    variance = X_train[numerical_cols].var()\n",
    "    \n",
    "    # Plot variance for each feature\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=variance, y=variance.index)\n",
    "    plt.title('Variance of Features')\n",
    "    plt.xlabel('Variance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.show()\n",
    "\n",
    "def plot_response_variable(apply_y_transformation=False):\n",
    "    \n",
    "    X_train, X_test, y_train = preprocessing_v1(apply_one_hot=True, apply_scaling=True, apply_remove_outliers=True)\n",
    "\n",
    "    if apply_y_transformation:\n",
    "        y_train = apply_log_transformation(y_train)\n",
    "        \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(y_train, kde=True)\n",
    "    plt.title('Distribution of Response Variable (PURITY)')\n",
    "    plt.xlabel('PURITY')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xlim(0, 10)\n",
    "    plt.show()\n",
    "\n",
    "def apply_log_transformation(y):\n",
    "    return np.log(y)\n",
    "\n",
    "def plot_boxplot(title, ax=None):\n",
    "    \"\"\"\n",
    "    Function to calculate summary statistics and plot a boxplot for numeric columns in a DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame containing the dataset.\n",
    "    - col_name: string, the column name of the feature to inspect.\n",
    "    - title: string, the title of the plot.\n",
    "    - ax: matplotlib axis object, allows the plot to be part of a larger figure.\n",
    "    \"\"\"\n",
    "    \n",
    "    X_train, X_test, y_train = preprocessing_v1(apply_one_hot=True, apply_scaling=True, apply_remove_outliers=True)\n",
    "\n",
    "    # Drop missing values\n",
    "    non_nan_series = y_train.dropna()\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    print(y_train.describe())\n",
    "    \n",
    "    # Check if the column is numeric\n",
    "    if pd.api.types.is_numeric_dtype(y_train):\n",
    "        \n",
    "        # Plot boxplot for numeric data\n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        ax.boxplot(non_nan_series, vert=False)\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel('PURITY')\n",
    "        ax.grid(True)\n",
    "        \n",
    "        # Show plot if standalone\n",
    "        if ax is None:\n",
    "            plt.show()\n",
    "    else:\n",
    "        print(f'PURITY is not a numeric column. Skipping boxplot.\\n')\n",
    " \n",
    "\n",
    "def calculate_feature_importance(X_train, y_train, X_test, threshold=0.25):\n",
    "    # Calculate feature importance using a linear regression model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Print feature importance\n",
    "    feature_importance = pd.Series(model.coef_, index=X_train.columns)\n",
    "    feature_importance = feature_importance.abs().sort_values(ascending=False)\n",
    "    wavelength_feature_importance_df = feature_importance.reset_index()\n",
    "    wavelength_feature_importance_df.columns = ['Feature', 'Importance']\n",
    "    wavelength_feature_importance_df.to_csv('/Users/maelysclerget/Desktop/ML/bio322_project/epfl-bio-322-2024/feature_importance_LR1.csv', index=False)\n",
    "    print('Feature Importance saved successfully.')\n",
    "    \n",
    "    # Calculate stats threshold\n",
    "    threshold_value = feature_importance.quantile(threshold)\n",
    "    \n",
    "    # Identify low-importance features\n",
    "    low_importance_features = feature_importance[feature_importance < threshold_value].index\n",
    "    print(f'Low importance features: {low_importance_features}')\n",
    "    \n",
    "    # Remove low-importance features\n",
    "    X_train_reduced = X_train.drop(columns=low_importance_features)\n",
    "    X_test_reduced = X_test.drop(columns=low_importance_features)\n",
    "    \n",
    "    return X_train_reduced, X_test_reduced\n",
    "\n",
    "def submission_file(y_test_predicted):\n",
    "    submission_reduced = pd.DataFrame({\n",
    "        'ID': range(1, len(y_test_predicted) + 1),\n",
    "        'PURITY': y_test_predicted\n",
    "    })\n",
    "    return submission_reduced\n",
    "\n",
    "\n",
    "def linear_regression(feature_importance=False, apply_y_transformation=False):\n",
    "    X_train, X_test, y_train = preprocessing_v1(apply_one_hot=True, apply_correlation=True, apply_scaling=True, apply_remove_outliers=True)\n",
    "    X_train = X_train.drop(columns=['sample_name'])\n",
    "    X_test = X_test.drop(columns=['sample_name'])\n",
    "    \n",
    "    if feature_importance:\n",
    "        X_train, X_test = calculate_feature_importance(X_train, y_train, X_test)\n",
    "        \n",
    "    if apply_y_transformation:\n",
    "        y_train = apply_log_transformation(y_train)\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_train_pred = model.predict(X_train)\n",
    "    mse = mean_squared_error(y_train, y_train_pred)\n",
    "    print('Training MSE:', mse)\n",
    "    \n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "    print('CV MSE:', -cv_scores.mean())\n",
    "    \n",
    "    # Predict on test data\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Create submission DataFrame\n",
    "    submission = submission_file(y_test_pred)\n",
    "    \n",
    "    # Save submission to CSV\n",
    "    submission.to_csv('/Users/maelysclerget/Desktop/ML/bio322_project/epfl-bio-322-2024/sample_submission_LR.csv', index=False)\n",
    "    print('Submission file saved successfully.')\n",
    "\n",
    "def polynomial_regression(apply_y_transformation=False):\n",
    "    X_train, X_test, y_train = preprocessing_v1(apply_one_hot=True, apply_correlation=True, apply_scaling=True, apply_remove_outliers=True)\n",
    "    X_train = X_train.drop(columns=['sample_name'])\n",
    "    X_test = X_test.drop(columns=['sample_name'])\n",
    "    \n",
    "    if apply_y_transformation:\n",
    "        y_train = apply_log_transformation(y_train)\n",
    "    \n",
    "    # Define the pipeline\n",
    "    pipeline = Pipeline([\n",
    "        (\"polynomial\", PolynomialFeatures()),\n",
    "        (\"regression\", LinearRegression())\n",
    "    ])\n",
    "    \n",
    "    # Define the parameter grid\n",
    "    param_grid = {\n",
    "        \"polynomial__degree\": np.arange(1, 2, 1)\n",
    "    }\n",
    "    \n",
    "    # Initialize GridSearchCV\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=5, \n",
    "                               scoring='neg_mean_squared_error', \n",
    "                               return_train_score=True)\n",
    "    \n",
    "    # Fit GridSearchCV\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "    print(\"Best accuracy:\", grid_search.best_score_)\n",
    "    print(\"Best parameters:\", grid_search.best_params_)\n",
    "    \n",
    "    # Predict on training data\n",
    "    y_train_pred = best_model.predict(X_train)\n",
    "    mse = mean_squared_error(y_train, y_train_pred)\n",
    "    print('Training MSE:', mse)\n",
    "    \n",
    "    # Cross-validation score\n",
    "    cv_scores = cross_val_score(best_model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "    print('CV MSE:', -cv_scores.mean())\n",
    "    \n",
    "    # Predict on test data\n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "    \n",
    "    # Create submission DataFrame\n",
    "    submission = submission_file(y_test_pred)\n",
    "    \n",
    "    # Save submission to CSV\n",
    "    submission.to_csv('/Users/maelysclerget/Desktop/ML/bio322_project/epfl-bio-322-2024/sample_submission_POLY_2.csv', index=False)\n",
    "    print('Submission file saved successfully.')\n",
    "    \n",
    "def ridge_regression(apply_y_transformation=False):\n",
    "    X_train, X_test, y_train = preprocessing_v1(apply_one_hot=True, apply_scaling=True, apply_remove_outliers=True, apply_correlation=True, apply_variance_threshold=False, apply_random_forest=False)\n",
    "    X_train = X_train.drop(columns=['sample_name'])\n",
    "    X_test = X_test.drop(columns=['sample_name'])\n",
    "    \n",
    "    if apply_y_transformation:\n",
    "        y_train = apply_log_transformation(y_train)\n",
    "        \n",
    "    # Define the pipeline\n",
    "    pipeline = Pipeline([\n",
    "        (\"polynomial\", PolynomialFeatures()),\n",
    "        (\"regression\", Ridge())\n",
    "    ])\n",
    "    \n",
    "    # Define the parameter grid\n",
    "    param_grid = {\n",
    "        \"polynomial__degree\": np.arange(1, 3, 1),\n",
    "        \"regression__alpha\": np.logspace(-12,-3,10)\n",
    "    }\n",
    "    \n",
    "    # Initialize GridSearchCV\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=5, \n",
    "                               scoring='neg_mean_squared_error', \n",
    "                               return_train_score=True)\n",
    "    \n",
    "    # Fit GridSearchCV\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "    print(\"Best accuracy:\", grid_search.best_score_)\n",
    "    print(\"Best parameters:\", grid_search.best_params_)\n",
    "    \n",
    "    # Predict on training data\n",
    "    y_train_pred = best_model.predict(X_train)\n",
    "    if apply_y_transformation:\n",
    "        y_train_pred = np.exp(y_train_pred)\n",
    "        y_train_original = np.exp(y_train)\n",
    "        mse = mean_squared_error(y_train_original, y_train_pred)\n",
    "    else:\n",
    "        mse = mean_squared_error(y_train, y_train_pred)\n",
    "    print('Training MSE:', mse)\n",
    "    \n",
    "    # Cross-validation score\n",
    "    cv_scores = cross_val_score(best_model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "    print('CV MSE:', -cv_scores.mean())\n",
    "    \n",
    "    # Predict on test data\n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "    if apply_y_transformation:\n",
    "        y_test_pred = np.exp(y_test_pred)\n",
    "    \n",
    "    # Create submission DataFrame\n",
    "    submission = submission_file(y_test_pred)\n",
    "    \n",
    "    # Save submission to CSV\n",
    "    submission.to_csv('/Users/maelysclerget/Desktop/ML/bio322_project/epfl-bio-322-2024/sample_submission_RIDGE.csv', index=False)\n",
    "    print('Submission file saved successfully.')\n",
    "    \n",
    "def lasso_regression(apply_y_transformation=False):\n",
    "    X_train, X_test, y_train = preprocessing_v1(apply_one_hot=True, apply_correlation=True, apply_scaling=True, apply_remove_outliers=True)\n",
    "    X_train = X_train.drop(columns=['sample_name'])\n",
    "    X_test = X_test.drop(columns=['sample_name'])\n",
    "    \n",
    "    if apply_y_transformation:\n",
    "        y_train = apply_log_transformation(y_train)\n",
    "        \n",
    "    # Define the pipeline\n",
    "    pipeline = Pipeline([\n",
    "        (\"polynomial\", PolynomialFeatures()),\n",
    "        (\"regression\", Lasso())\n",
    "    ])\n",
    "    \n",
    "    # Define the parameter grid\n",
    "    param_grid = {\n",
    "        \"polynomial__degree\": np.arange(1, 3, 1),\n",
    "        \"regression__alpha\": np.logspace(-12, -3, 10)\n",
    "    }\n",
    "    \n",
    "    # Initialize GridSearchCV\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=10, \n",
    "                               scoring='neg_mean_squared_error', \n",
    "                               return_train_score=True)\n",
    "    \n",
    "    # Fit GridSearchCV\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "    print(\"Best accuracy:\", grid_search.best_score_)\n",
    "    print(\"Best parameters:\", grid_search.best_params_)\n",
    "    \n",
    "    # Predict on training data\n",
    "    y_train_pred = best_model.predict(X_train)\n",
    "    if apply_y_transformation:\n",
    "        y_train_pred = np.exp(y_train_pred)\n",
    "        y_train_original = np.exp(y_train)\n",
    "        mse = mean_squared_error(y_train_original, y_train_pred)\n",
    "    else:\n",
    "        mse = mean_squared_error(y_train, y_train_pred)\n",
    "    print('Training MSE:', mse)\n",
    "    \n",
    "    # Cross-validation score\n",
    "    cv_scores = cross_val_score(best_model, X_train, y_train, cv=10, scoring='neg_mean_squared_error')\n",
    "    print('CV MSE:', -cv_scores.mean())\n",
    "    \n",
    "    # Predict on test data\n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "    if apply_y_transformation:\n",
    "        y_test_pred = np.exp(y_test_pred)\n",
    "    \n",
    "    # Create submission DataFrame\n",
    "    submission = submission_file(y_test_pred)\n",
    "    \n",
    "    # Save submission to CSV\n",
    "    submission.to_csv('/Users/maelysclerget/Desktop/ML/bio322_project/epfl-bio-322-2024/sample_submission_LASSO.csv', index=False)\n",
    "    print('Submission file saved successfully.')\n",
    "\n",
    "def elasticnet_regression(apply_y_transformation=False):\n",
    "    X_train, X_test, y_train = preprocessing_v1(apply_one_hot=True, apply_correlation=True, apply_scaling=True, apply_remove_outliers=True)\n",
    "    X_train = X_train.drop(columns=['sample_name'])\n",
    "    X_test = X_test.drop(columns=['sample_name'])\n",
    "    \n",
    "    if apply_y_transformation:\n",
    "        y_train = apply_log_transformation(y_train)\n",
    "        \n",
    "    # Define the pipeline\n",
    "    pipeline = Pipeline([\n",
    "        (\"polynomial\", PolynomialFeatures()),\n",
    "        (\"regression\", ElasticNet())\n",
    "    ])\n",
    "    \n",
    "    # Define the parameter grid\n",
    "    param_grid = {\n",
    "        \"polynomial__degree\": np.arange(1, 3, 1),\n",
    "        \"regression__alpha\": np.logspace(-12, -3, 10),\n",
    "        \"regression__l1_ratio\": np.linspace(0, 1, 10)\n",
    "    }\n",
    "    \n",
    "    # Initialize GridSearchCV\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=5, \n",
    "                               scoring='neg_mean_squared_error', \n",
    "                               return_train_score=True)\n",
    "    \n",
    "    # Fit GridSearchCV\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "    print(\"Best accuracy:\", grid_search.best_score_)\n",
    "    print(\"Best parameters:\", grid_search.best_params_)\n",
    "    \n",
    "    # Predict on training data\n",
    "    y_train_pred = best_model.predict(X_train)\n",
    "    if apply_y_transformation:\n",
    "        y_train_pred = np.exp(y_train_pred)\n",
    "        y_train_original = np.exp(y_train)\n",
    "        mse = mean_squared_error(y_train_original, y_train_pred)\n",
    "    else:\n",
    "        mse = mean_squared_error(y_train, y_train_pred)\n",
    "    print('Training MSE:', mse)\n",
    "    \n",
    "    # Cross-validation score\n",
    "    cv_scores = cross_val_score(best_model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "    print('CV MSE:', -cv_scores.mean())\n",
    "    \n",
    "    # Predict on test data\n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "    if apply_y_transformation:\n",
    "        y_test_pred = np.exp(y_test_pred)\n",
    "    \n",
    "    # Create submission DataFrame\n",
    "    submission = submission_file(y_test_pred)\n",
    "    \n",
    "    # Save submission to CSV\n",
    "    submission.to_csv('/Users/maelysclerget/Desktop/ML/bio322_project/epfl-bio-322-2024/sample_submission_ELASTICNET.csv', index=False)\n",
    "    print('Submission file saved successfully.')\n",
    "    \n",
    "def bayesian_ridge_regression(apply_y_transformation=False):\n",
    "    X_train, X_test, y_train = preprocessing_v1(apply_one_hot=True, apply_correlation=False, apply_scaling=True, apply_remove_outliers=False, apply_random_forest=True)\n",
    "    X_train = X_train.drop(columns=['sample_name'])\n",
    "    X_test = X_test.drop(columns=['sample_name'])\n",
    "    \n",
    "    if apply_y_transformation:\n",
    "        y_train = apply_log_transformation(y_train)\n",
    "    \n",
    "    # Define the objective function for Bayesian Optimization\n",
    "    def objective(alpha_1, alpha_2, lambda_1, lambda_2):\n",
    "        model = BayesianRidge(alpha_1=alpha_1, alpha_2=alpha_2, lambda_1=lambda_1, lambda_2=lambda_2)\n",
    "        cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "        return -cv_scores.mean()\n",
    "    \n",
    "    # Define the parameter bounds\n",
    "    params_bayesian_ridge = {\n",
    "        'alpha_1': (1e-6, 1e-3),\n",
    "        'alpha_2': (1e-6, 1e-3),\n",
    "        'lambda_1': (1e-6, 1e-3),\n",
    "        'lambda_2': (1e-6, 1e-3)\n",
    "    }\n",
    "    \n",
    "    # Initialize Bayesian Optimization\n",
    "    optimizer = BayesianOptimization(f=objective, pbounds=params_bayesian_ridge, random_state=42)\n",
    "    \n",
    "    # Maximize the objective function\n",
    "    optimizer.maximize(init_points=10, n_iter=50)\n",
    "    \n",
    "    # Get the best parameters\n",
    "    best_params = optimizer.max['params']\n",
    "    print(\"Best parameters:\", best_params)\n",
    "    \n",
    "    # Train the final model with the best parameters\n",
    "    bayesian_ridge = BayesianRidge(**best_params)\n",
    "    bayesian_ridge.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on training data\n",
    "    y_train_pred = bayesian_ridge.predict(X_train)\n",
    "    if apply_y_transformation:\n",
    "        y_train_pred = np.expm1(y_train_pred)\n",
    "        y_train_original = np.expm1(y_train)\n",
    "        mse = mean_squared_error(y_train_original, y_train_pred)\n",
    "    else:\n",
    "        mse = mean_squared_error(y_train, y_train_pred)\n",
    "    print('Training MSE:', mse)\n",
    "    \n",
    "    # Cross-validation score\n",
    "    cv_scores = cross_val_score(bayesian_ridge, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "    print('CV MSE:', -cv_scores.mean())\n",
    "    \n",
    "    # Predict on test data\n",
    "    y_test_pred = bayesian_ridge.predict(X_test)\n",
    "    if apply_y_transformation:\n",
    "        y_test_pred = np.expm1(y_test_pred)\n",
    "    \n",
    "    # Create submission DataFrame\n",
    "    submission = submission_file(y_test_pred)\n",
    "    \n",
    "    # Save submission to CSV\n",
    "    submission.to_csv('/Users/maelysclerget/Desktop/ML/bio322_project/epfl-bio-322-2024/sample_submission_BAYESIAN_RIDGE.csv', index=False)\n",
    "    print('Submission file saved successfully.')\n",
    "\n",
    "def huber_regression(apply_feature_importance=True, apply_y_transformation=True):\n",
    "    X_train, X_test, y_train = preprocessing_v1(apply_one_hot=True, apply_correlation=False, apply_scaling=True, apply_remove_outliers=True, apply_variance_threshold=False, apply_random_forest=True)\n",
    "    \n",
    "    X_train = X_train.drop(columns=['sample_name'])\n",
    "    X_test = X_test.drop(columns=['sample_name'])\n",
    "    \n",
    "    if apply_feature_importance:\n",
    "        X_train, X_test = calculate_feature_importance(X_train, y_train, X_test)\n",
    "    \n",
    "    if apply_y_transformation:\n",
    "        y_train = apply_log_transformation(y_train)\n",
    "        \n",
    "    # Define the parameter grid for epsilon\n",
    "    param_grid = {'epsilon': np.arange(1, 2, 0.5)}\n",
    "     \n",
    "    # Initialize the HuberRegressor\n",
    "    huber = HuberRegressor()\n",
    "    \n",
    "    # Initialize GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=huber, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "    \n",
    "    # Fit GridSearchCV\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the best model\n",
    "    best_huber = grid_search.best_estimator_\n",
    "    print(\"Best parameters:\", grid_search.best_params_)\n",
    "    \n",
    "    # Predict on training data\n",
    "    y_train_pred = best_huber.predict(X_train)\n",
    "    if apply_y_transformation:\n",
    "        y_train_pred = np.exp(y_train_pred)\n",
    "        y_train_original = np.exp(y_train)\n",
    "        mse = mean_squared_error(y_train_original, y_train_pred)\n",
    "    else:\n",
    "        mse = mean_squared_error(y_train, y_train_pred)\n",
    "    print('Training MSE:', mse)\n",
    "    \n",
    "    # Cross-validation score\n",
    "    cv_scores = cross_val_score(best_huber, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "    print('CV MSE:', -cv_scores.mean())\n",
    "    \n",
    "    # Predict on test data\n",
    "    y_test_pred = best_huber.predict(X_test)\n",
    "    if apply_y_transformation:\n",
    "        y_test_pred = np.exp(y_test_pred)\n",
    "    \n",
    "    # Create submission DataFrame\n",
    "    submission = submission_file(y_test_pred)\n",
    "    \n",
    "    # Save submission to CSV\n",
    "    submission.to_csv('/Users/maelysclerget/Desktop/ML/bio322_project/epfl-bio-322-2024/sample_submission_HUBER.csv', index=False)\n",
    "    print('Submission file saved successfully.')\n",
    "    \n",
    "def orthogonal_matching_pursuit():\n",
    "    \n",
    "    X_train, X_test, y_train = preprocessing_v1(apply_one_hot=True, apply_correlation=False, apply_scaling=True, apply_remove_outliers=True, apply_variance_threshold=False, apply_random_forest=False)\n",
    "    \n",
    "    X_train = X_train.drop(columns=['sample_name'])\n",
    "    X_test = X_test.drop(columns=['sample_name'])\n",
    "    \n",
    "    # Define the pipeline\n",
    "    pipeline = Pipeline([\n",
    "        (\"polynomial\", PolynomialFeatures()),\n",
    "        (\"regression\", OrthogonalMatchingPursuit())\n",
    "    ])\n",
    "    \n",
    "    # Define the parameter grid\n",
    "    param_grid = {\n",
    "        \"polynomial__degree\": np.arange(1, 3, 1),\n",
    "        \"regression__n_nonzero_coefs\": range(1, 100, 10)\n",
    "    }\n",
    "    \n",
    "    # Initialize GridSearchCV\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=5, \n",
    "                               scoring='neg_mean_squared_error', \n",
    "                               return_train_score=True)\n",
    "    \n",
    "    # Fit GridSearchCV\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "    print(\"Best accuracy:\", grid_search.best_score_)\n",
    "    print(\"Best parameters:\", grid_search.best_params_)\n",
    "    \n",
    "    # Predict on training data\n",
    "    y_train_pred = best_model.predict(X_train)\n",
    "    mse = mean_squared_error(y_train, y_train_pred)\n",
    "    print('Training MSE:', mse)\n",
    "    \n",
    "    # Cross-validation score\n",
    "    cv_scores = cross_val_score(best_model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "    print('CV MSE:', -cv_scores.mean())\n",
    "    \n",
    "    # Predict on test data\n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "    \n",
    "    # Create submission DataFrame\n",
    "    submission = submission_file(y_test_pred)\n",
    "    \n",
    "    # Save submission to CSV\n",
    "    submission.to_csv('/Users/maelysclerget/Desktop/ML/bio322_project/epfl-bio-322-2024/sample_submission_OMP.csv', index=False)\n",
    "    print('Submission file saved successfully.')\n",
    "\n",
    "def random_forest_linear_regression(apply_y_transformation=False):\n",
    "    \n",
    "    X_train, X_test, y_train = preprocessing_v1(apply_one_hot=True, apply_correlation=False, apply_scaling=True, apply_remove_outliers=True, apply_variance_threshold=False, apply_random_forest=True)\n",
    "    \n",
    "    X_train = X_train.drop(columns=['sample_name'])\n",
    "    X_test = X_test.drop(columns=['sample_name'])\n",
    "    \n",
    "    if apply_y_transformation:\n",
    "        y_train = np.log1p(y_train)  # Apply log transformation\n",
    "        \n",
    "    # Initialize the RandomForestRegressor\n",
    "    rf = RandomForestRegressor(n_estimators=300, random_state=42)\n",
    "    \n",
    "    # Fit the model\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on training data\n",
    "    y_train_pred = rf.predict(X_train)\n",
    "    if apply_y_transformation:\n",
    "        y_train_pred = np.exp(y_train_pred)  # Apply inverse log transformation\n",
    "        y_train_original = np.exp(y_train)\n",
    "        mse = mean_squared_error(y_train_original, y_train_pred)\n",
    "    else:\n",
    "        mse = mean_squared_error(y_train, y_train_pred)\n",
    "    print('Training MSE:', mse)\n",
    "    \n",
    "    # Cross-validation score\n",
    "    cv_scores = cross_val_score(rf, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "    print('CV MSE:', -cv_scores.mean())\n",
    "    \n",
    "    # Predict on test data\n",
    "    y_test_pred = rf.predict(X_test)\n",
    "    if apply_y_transformation:\n",
    "        y_test_pred = np.exp(y_test_pred)  # Apply inverse log transformation\n",
    "    \n",
    "    # Create submission DataFrame\n",
    "    submission = submission_file(y_test_pred)\n",
    "    \n",
    "    # Save submission to CSV\n",
    "    submission.to_csv('/Users/maelysclerget/Desktop/ML/bio322_project/sample_submission_RF.csv', index=False)\n",
    "    print('Submission file saved successfully.')\n",
    "     \n",
    "def main():\n",
    "    #linear_regression(feature_importance=True)\n",
    "    #polynomial_regression()\n",
    "    #ridge_regression()\n",
    "    #plot_response_variable(True)\n",
    "    #plot_boxplot(\"Hérisson\")\n",
    "    #lasso_regression(apply_y_transformation=True)\n",
    "    #elasticnet_regression()\n",
    "    #plot_variance()\n",
    "    #best_param_RF()\n",
    "    #bayesian_ridge_regression()\n",
    "    #huber_regression()\n",
    "    #orthogonal_matching_pursuit()\n",
    "    random_forest_linear_regression()\n",
    "   \n",
    "if __name__ == '__main__':\n",
    "    main() \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLCourse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
